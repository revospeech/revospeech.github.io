---
title: RevoSpeech 智能语音白皮书
date: 2023-02-12 11:40:36
tags: [ASR, TTS]
categories: 技术思考
copyright_info: true
toc: true
mathjax: true
comment: false
# aging: false
# aging_days: 200
sticky: 999
---

智能语音是当今科技发展的热门方向之一。随着人工智能技术的不断进步，智能语音技术日趋成熟，在各个领域的应用也在不断增多，目前已在语音搜索、智能家居、智能语音助理等多个领域得进行落地，并且随着元宇宙、AIGC 等新产业的兴起焕发出新的活力。[**RevoSpeech**](https://revospeech.github.io) 旨在推动智能语音的落地和普及，基于学术界近十年在语音处理、语音识别、语音合成等方向的技术突破，深入探讨语音识别和语音合成技术发展，总结归纳智能语音的技术要点，密切跟进前沿科研动向的同时，展望未来智能语音乃至人工智能技术将何去何从。

<!-- more -->

本文主要梳理未来一年（甚至更长时间范围）内，RevoSpeech 计划在智能语音方向的发展指南（Road Map），也可以称之为**智能语音白皮书**，主要着眼于技术总结和前沿论文跟进，同时也将对智能语音领域的数据及工程落地等问题进行探讨。

> 注意：本白皮书将主要着眼于**语音识别**和**语音合成**两大技术。

# 语音识别

## 语音数据库构建

### 开源语音数据梳理

- 整理目前语音社区内的大规模语音数据，持续跟进开源数据
- 覆盖英文、中文、韩语、日语、俄语、法语等多语种语音数据
- **持续更新中**：[speech-datasets-collection](https://github.com/RevoSpeechTech/speech-datasets-collection)

### 自建语音数据库

#### 数据库构建流程

- 参考论文
	- LibriSpeech: [pdf](http://www.danielpovey.com/files/2015_icassp_librispeech.pdf), MLS (LibriVox): [pdf](https://arxiv.org/pdf/2012.03411.pdf)
	- GigaSpeech: [pdf](https://arxiv.org/pdf/2106.06909.pdf), WenetSpeech: [pdf](https://arxiv.org/pdf/2110.03370.pdf)
	- JTubeSpeech: [pdf](https://arxiv.org/pdf/2112.09323.pdf), SPGISpeech: [pdf](https://arxiv.org/pdf/2104.02014.pdf), The People's Speech: [pdf](https://arxiv.org/pdf/2111.09344.pdf)

- 数据获取来源
	- Videos: [YouTube](https://www.youtube.com), [bilibili](https://www.bilibili.com)
	- Podcasts: [google](https://podcasts.google.com), [apple](https://www.apple.com/hk/en/apple-podcasts)...
	- Misc: [archive.org](https://archive.org)...

- 数据获取工具
	- [ytb-dlp](https://github.com/yt-dlp/yt-dlp), [you-get](https://github.com/soimort/you-get)

- 数据清洗流程（待梳理）
	- 文本归一化: [NeMo](https://github.com/NVIDIA/NeMo-text-processing), [Wenet](https://github.com/wenet-e2e/WeTextProcessing), [SpeechIO](https://github.com/speechio/chinese_text_normalization)
	- 强制对齐: [Kaldi](https://github.com/kaldi-asr/kaldi), CTC, [DSAlign](https://github.com/mozilla/DSAlign)
	- 音频切分与校验：[Kaldi cleanup](https://github.com/kaldi-asr/kaldi/blob/master/egs/wsj/s5/steps/cleanup/clean_and_segment_data_nnet3.sh)


#### 数据库构建计划
**_v1.0 阶段_**

- 目标语言：中文/英文
- 数据来源：youtube / bilibili / podcast 自带字幕文件的音视频数据
- 对齐模型：kaldi 使用 multi-cn + wenetspeech 的 nnet3 模型
- 处理思路：基于字幕或文本进行长音频切分并进行 cleanup 处理
- 方案评测：抽查数据准确度 (要求 97% 以上)，确认方案可行性
- 最终产出：中文/英文各 10k 小时以上

**_v2.0 阶段_**

- 目标语言：中文/英文
- 数据来源：youtube / bilibili 不带外挂字幕但存在硬字幕的视频
- 额外功能：需要额外进行字幕定位和 OCR 文本识别功能
- 处理思路：以 OCR 识别出的结果为伪标签，再进行 cleanup 处理
- 方案评测：抽查数据准确度 (要求 97% 以上)，确认方案可行性
- 最终产出：中文/英文各 10k 小时以上

**_v3.0 阶段_** (长期计划)

- 多次迭代模型，提高 Kaldi 中文/英文对齐模型的多领域泛化能力
- 扩充目标一：中文/英文的不同方言/口音的语音数据
- 扩充目标二：法语/德语/西班牙/韩语/日语 等多语种的语音数据
- 扩充目标三：Audio-Visual 语音和图像多模态的数据库

---

## 传统语音识别

以 **Kaldi** 为代表的传统语音识别，目前在学术界/产业界仍然保持着相对的优势。虽然识别准确率与端到端模型已经存在比较明显的差距，但其中声学特征、MMI 损失函数、WFST 解码器、语言模型重打分等构件在端到端语音识别中仍然是重要的技术。因此，在传统语音识别部分，RevoSpeech 将着力推进 Kaldi-Revo 项目，优化传统语音识别的训练框架，充分发挥其模型轻量、落地成本低等优势。

### 训练流程优化

#### 声学特征
- 理论梳理：MFCC/FBank/LPC 等声学特征的提取流程
- 代码研读：Kaldi 特征提取部分的代码整理
- 工程实践：C++ 动态库及接口封装知识

#### 声学模型

- 核心训练流程：GMM-HMM → DNN-HMM(nnet3) → LF-MMI(chain) 
- 发音词典构建：在 GMM 早期加入更多可能的发音，统计训练数据的发音概率进行筛选
- 前沿声学模型：
	- 候选结构：TDNN-F-SAN 或 Multi-Stream TDNN-F-SAN
	- 流式识别：保证实时性、准确性、时延低

#### 语言模型
- N-gram 语言模型：
	- 通用向语言模型的训练
- 语言模型重打分：
	- 候选结构：Transformer/Transformer-XL 两种模型的双向重打分
	- 加速思想：词级别语言模型 → 字级别语言模型
	- 工程实践：Transformer-XL 参考 NVIDIA 的 [Benchmark](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/Transformer-XL)

#### 解码器
- 理论学习：
	- 参考教程：语音识别：原理与应用(第二版), [代码研读](https://github.com/snsun/kaldi-decoder-code-reading)
- 热词方案：基于前缀树的方案在 Kaldi 解码器代码上实现
- 解码器复现
	- HMM 的 WFST Decoder 的实现方法
	- CTC 的 WFST Decoder 的实现方法
- 解码器优化
	- BigLM Decoder 的实现方法
	- [Async Decoder](https://www.danielpovey.com/files/2021_icassp_async_biglm_decoder.pdf)
	- [LET-Decoder](https://www.danielpovey.com/files/2021_spl_lazy_evaluation_decoder.pdf)

### 开源数据集实验
**_v1.0 阶段_**

- 中文：aishell / aidatatang_200zh / MagicData-RAMC / multi_cn / WenetSpeech
- 英文：LibriSpeech / TedLium / GigaSpeech
- 中英 Code-Switching：ASCEND / TALCS
- 英文：尝试 **grapheme 级别的 chenone** 建模方式 [pdf](https://arxiv.org/pdf/1910.01493.pdf)
- 产出：整理对比实验结果，梳理得到 Benchmark

**_v2.0 阶段_**

- 自监督特征：Wav2vec2 / WavLM / HuBert / Data2vec
- 多通道/远场：aishell4 / alimeeting / [CHiME(5|6|7)](https://www.chimechallenge.org)
- 多语种 ASR

**_v3.0 阶段_**(长期计划)
- 多模态 (AVSR)：LRS2-BBC / LRS3-TED / [CMLR](https://www.vipazoo.cn/CMLR.html)
- 多模态预训练模型特征：AV-HuBert
- 歌词识别/转写：DAMP / DALI
- 多模态歌词识别：[N20EM](https://n20em.github.io)
- 其他参考工作：[pkwrap](https://github.com/idiap/pkwrap), [apam](https://github.com/idiap/apam), [wav2vec-lfmmi](https://github.com/idiap/wav2vec-lfmmi)


### 工具化实践
- CPU 服务：vosk-server / vosk-api 实践及优化
- GPU 服务：Nvidia Triton GPU 服务化实践及结论整理
- Android 端侧：Vosk-android 实践及优化
- 参考书籍：语音识别服务实战

---

## 端到端语音识别

TODO