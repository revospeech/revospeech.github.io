<!DOCTYPE html><html lang="zh-CN"><head><meta name="google-site-verification" content="YCdLZ428Qxbc1A-LthSxDRD0i9qhYVAMj6s4vMllnOs"><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="keywords" content="Hexo Theme Keep"><meta name="description" content="Hexo Theme Keep"><meta name="author" content="RevoSpeech"><title>论文集 | 音频生成前沿论文 | RevoSpeech</title><link rel="stylesheet" href="/css/style.css"><link rel="shortcut icon" href="https://cdn.staticaly.com/gh/revospeech/image-hosting@master/20230228/favicon_white.jpg"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/font/css/fontawesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/font/css/regular.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/font/css/solid.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/font/css/brands.min.css"><script id="hexo-configurations">let KEEP=window.KEEP||{};KEEP.hexo_config={hostname:"revospeech.github.io",root:"/",language:"zh-CN",path:"search.json"},KEEP.theme_config={toc:{enable:!0,number:!0,expand_all:!0,init_open:!0},style:{primary_color:"#0066cc",logo:"https://cdn.staticaly.com/gh/revospeech/image-hosting@master/20230211/R-scale.jpg",favicon:"https://cdn.staticaly.com/gh/revospeech/image-hosting@master/20230228/favicon_white.jpg",avatar:"https://cdn.staticaly.com/gh/revospeech/image-hosting@master/20230211/avatar.mn776y46kc0.jpg",font_size:null,font_family:null,hover:{shadow:!0,scale:!0},first_screen:{enable:!0,header_transparent:!1,background_img:"https://cdn.staticaly.com/gh/revospeech/image-hosting@master/20230212/background-mask-grey.jpg",description:"Revive and Evolve || Towards the Future",font_color:"#E9E9E9",hitokoto:!1},scroll:{progress_bar:!0,percent:!0}},local_search:{enable:!0,preload:!0},code_copy:{},code_block:{tools:{enable:!0,style:"default"},highlight_theme:"obsidian"},side_tools:{},pjax:{enable:!0},lazyload:{enable:!0},comment:{enable:!1,use:"waline",valine:{appid:null,appkey:null,server_urls:null,placeholder:null},gitalk:{github_id:null,github_admins:null,repository:null,client_id:null,client_secret:null,proxy:null},twikoo:{env_id:null,region:null,version:"1.6.8"},waline:{server_url:"https://ixvtasha.api.lncldglobal.com",reaction:!1,version:2}},post:{author_label:{enable:!0,auto:!0,custom_label_list:["Trainee","Engineer","Architect"]},word_count:{enable:!0,wordcount:!0,min2read:!0},img_align:"center",copyright_info:!0},version:"3.6.1"},KEEP.language_ago={second:"%s 秒前",minute:"%s 分钟前",hour:"%s 小时前",day:"%s 天前",week:"%s 周前",month:"%s 个月前",year:"%s 年前"},KEEP.language_code_block={copy:"复制代码",copied:"已复制",fold:"折叠代码块",folded:"已折叠"},KEEP.language_copy_copyright={copy:"复制版权信息",copied:"已复制",title:"原文标题",author:"原文作者",link:"原文链接"}</script><meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="alternate" href="/atom.xml" title="RevoSpeech" type="application/atom+xml">
</head><body><div class="progress-bar-container"><span class="scroll-progress-bar"></span> <span class="pjax-progress-bar"></span> <i class="pjax-progress-icon fas fa-circle-notch fa-spin"></i></div><main class="page-container"><div class="page-main-content"><div class="page-main-content-top"><header class="header-wrapper"><div class="header-content"><div class="left"><a class="logo-image" href="/"><img src="https://cdn.staticaly.com/gh/revospeech/image-hosting@master/20230211/R-scale.jpg"> </a><a class="logo-title" href="/">RevoSpeech</a></div><div class="right"><div class="pc"><ul class="menu-list"><li class="menu-item"><a href="/">首页</a></li><li class="menu-item"><a href="/archives">归档</a></li><li class="menu-item"><a href="/tags">标签</a></li><li class="menu-item"><a href="/about">关于</a></li><li class="menu-item search search-popup-trigger"><i class="fas fa-search"></i></li></ul></div><div class="mobile"><div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div><div class="icon-item menu-bar"><div class="menu-bar-middle"></div></div></div></div></div><div class="header-drawer"><ul class="drawer-menu-list"><li class="drawer-menu-item flex-center"><a href="/">首页</a></li><li class="drawer-menu-item flex-center"><a href="/archives">归档</a></li><li class="drawer-menu-item flex-center"><a href="/tags">标签</a></li><li class="drawer-menu-item flex-center"><a href="/about">关于</a></li></ul></div><div class="window-mask"></div></header></div><div class="page-main-content-middle"><div class="main-content"><div class="fade-in-down-animation"><div class="page-template-container"><div class="page-template-content keep-markdown-body"><table><thead><tr><th align="center"><strong>Year</strong></th><th align="center"><strong>Organization</strong></th><th align="center"><strong>Name</strong></th><th align="center"><strong>Title</strong></th><th align="center"><strong>Paper</strong></th><th align="center"><strong>Demo</strong></th><th align="center"><strong>Code</strong></th></tr></thead><tbody><tr><td align="center">2020</td><td align="center">OpenAI</td><td align="center">Jukebox</td><td align="center">Jukebox: A Generative Model for Music</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2005.00341.pdf">[2005.00341]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://openai.com/research/jukebox">[demo]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/openai/jukebox">[code]<i class="fas fa-external-link-alt"></i></a></td></tr><tr><td align="center">2021</td><td align="center">Google</td><td align="center">Soundstream</td><td align="center">Soundstream: An end-to-end neural audio codec</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2107.03312.pdf">[2107.03312]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://google-research.github.io/seanet/soundstream/examples/">[demo]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/google/lyra">[code-1]<i class="fas fa-external-link-alt"></i></a><br><a class="link" target="_blank" rel="noopener" href="https://github.com/wesbz/SoundStream">[code-2]<i class="fas fa-external-link-alt"></i></a></td></tr><tr><td align="center">2021</td><td align="center">IRCAM</td><td align="center">RAVE</td><td align="center">RAVE: A variational autoencoder for fast and high-quality neural audio synthesis</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2111.05011.pdf">[2111.05011]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://anonymous84654.github.io/RAVE_anonymous/">[demo]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/acids-ircam/RAVE">[code]<i class="fas fa-external-link-alt"></i></a></td></tr><tr><td align="center">2022</td><td align="center">Google</td><td align="center">Perceiver-AR</td><td align="center">General-purpose, long-context autoregressive modeling with Perceiver AR</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2202.07765.pdf">[2202.07765]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://storage.googleapis.com/perceiver-ar/index.html">[demo]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/google-research/perceiver-ar">[code-1]<i class="fas fa-external-link-alt"></i></a><br><a class="link" target="_blank" rel="noopener" href="https://github.com/asigalov61/Perceiver-Music-Transformer">[code-2]<i class="fas fa-external-link-alt"></i></a></td></tr><tr><td align="center">2022</td><td align="center">Stanford</td><td align="center">SASHIMI</td><td align="center">It’s raw! audio generation with state-space models</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2202.09729.pdf">[2202.09729]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://hazyresearch.stanford.edu/sashimi-examples/">[demo]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/albertfgu/diffwave-sashimi">[code]<i class="fas fa-external-link-alt"></i></a></td></tr><tr><td align="center">2022</td><td align="center">Baidu</td><td align="center">A3T</td><td align="center">A3T: Alignment-Aware Acoustic and Text Pretraining for Speech Synthesis and Editing</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.09690.pdf">[2203.09690]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/richardbaihe/a3t">[demo]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/richardbaihe/a3t">[code]<i class="fas fa-external-link-alt"></i></a></td></tr><tr><td align="center">2022</td><td align="center">SJTU</td><td align="center">VQTTS</td><td align="center">VQTTS: High-Fidelity Text-to-Speech Synthesis with Self-Supervised VQ Acoustic Feature</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.00768.pdf">[2204.00768]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://cpdu.github.io/vqtts/">[demo]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/vliu15/speech-masters-thesis">[code]<i class="fas fa-external-link-alt"></i></a></td></tr><tr><td align="center">2022</td><td align="center">Google</td><td align="center">Spectrogram Diffusion</td><td align="center">Multi-instrument Music Synthesis with Spectrogram Diffusion</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2206.05408.pdf">[2206.05408]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://colab.research.google.com/github/magenta/music-spectrogram-diffusion/blob/main/music_spectrogram_diffusion/colab/synthesize_midi.ipynb">[demo]<i class="fas fa-external-link-alt"></i></a></td><td align="center">-</td></tr><tr><td align="center">2022</td><td align="center">Microsoft</td><td align="center">DelightfulTTS 2</td><td align="center">DelightfulTTS 2: End-to-End Speech Synthesis with Adversarial Vector-Quantized Auto-Encoders</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.04646.pdf">[2207.04646]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://cognitivespeech.github.io/delightfultts2">[demo]<i class="fas fa-external-link-alt"></i></a></td><td align="center">-</td></tr><tr><td align="center">2022</td><td align="center">Google</td><td align="center">MuLan</td><td align="center">Mulan: A joint embedding of music audio and natural language</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2208.12415.pdf">[2208.12415]<i class="fas fa-external-link-alt"></i></a></td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/lucidrains/musiclm-pytorch/blob/main/musiclm_pytorch/musiclm_pytorch.py">[code]<i class="fas fa-external-link-alt"></i></a></td></tr><tr><td align="center">2022</td><td align="center">Google</td><td align="center">AudioLM</td><td align="center">AudioLM: a Language Modeling Approach to Audio Generation</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2209.03143.pdf">[2209.03143]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://google-research.github.io/seanet/audiolm/examples/">[demo]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/lucidrains/audiolm-pytorch">[code]<i class="fas fa-external-link-alt"></i></a></td></tr><tr><td align="center">2022</td><td align="center">Meta AI</td><td align="center">AudioGen</td><td align="center">AudioGen: Textually Guided Audio Generation</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2209.15352.pdf">[2209.15352]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://felixkreuk.github.io/audiogen/">[demo]<i class="fas fa-external-link-alt"></i></a></td><td align="center">-</td></tr><tr><td align="center">2022</td><td align="center">Microsoft</td><td align="center">Museformer</td><td align="center">Museformer: Transformer with Fine- and Coarse-Grained Attention for Music Generation</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2210.10349.pdf">[2210.10349]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://ai-muzic.github.io/museformer/">[demo]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/microsoft/muzic/tree/main/museformer">[code]<i class="fas fa-external-link-alt"></i></a></td></tr><tr><td align="center">2022</td><td align="center">Meta AI</td><td align="center">Encodec</td><td align="center">High Fidelity Neural Audio Compression</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2210.13438.pdf">[2210.13438]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://ai.honu.io/papers/encodec/samples.html">[demo]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/facebookresearch/encodec">[code]<i class="fas fa-external-link-alt"></i></a></td></tr><tr><td align="center">2022</td><td align="center">Meta AI</td><td align="center">Modified AudioGen</td><td align="center">Audio Language Modeling using Perceptually-Guided Discrete Representations</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2211.01223.pdf">[2211.01223]<i class="fas fa-external-link-alt"></i></a></td><td align="center">-</td><td align="center">-</td></tr><tr><td align="center">2022</td><td align="center">Baidu</td><td align="center">ERNIE-SAT</td><td align="center">ERNIE-SAT: Speech and Text Joint Pretraining for Cross-Lingual Multi-Speaker Text-to-Speech</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2211.03545.pdf">[2211.03545]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://brainy-candy-0a2.notion.site/ERNIE-SAT-DEMO-6f3ef7fbea944d9db46ba2770ab6693d">[demo]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/examples/aishell3_vctk/ernie_sat">[code]<i class="fas fa-external-link-alt"></i></a></td></tr><tr><td align="center">2023</td><td align="center">Microsoft</td><td align="center">PromptTTS</td><td align="center">PromptTTS: Controllable Text-to-Speech with Text Descriptions</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2211.12171.pdf">[2211.12171]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://speechresearch.github.io/prompttts/">[demo]<i class="fas fa-external-link-alt"></i></a></td><td align="center">-</td></tr><tr><td align="center">2023</td><td align="center">Microsoft</td><td align="center">VALL-E</td><td align="center">Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2301.02111.pdf">[2301.02111]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://valle-demo.github.io/">[demo]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/enhuiz/vall-e">[code]<i class="fas fa-external-link-alt"></i></a></td></tr><tr><td align="center">2023</td><td align="center">-</td><td align="center">Msanii</td><td align="center">Msanii: High Fidelity Music Synthesis on a Shoestring Budget</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2301.06468.pdf">[2301.06468]<i class="fas fa-external-link-alt"></i></a></td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/Kinyugo/msanii">[code]<i class="fas fa-external-link-alt"></i></a></td></tr><tr><td align="center">2023</td><td align="center">Google</td><td align="center">MusicLM</td><td align="center">MusicLM: Generating Music From Text</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2301.11325.pdf">[2301.11325]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://google-research.github.io/seanet/musiclm/examples/">[demo]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/lucidrains/musiclm-pytorch">[code]<i class="fas fa-external-link-alt"></i></a></td></tr><tr><td align="center">2023</td><td align="center">ETH</td><td align="center">Moûsai</td><td align="center">Moûsai: Text-to-Music Generation with Long-Context Latent Diffusion</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2301.11757.pdf">[2301.11757]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://flavioschneider.notion.site/flavioschneider/Audio-Generation-with-Diffusion-c4f29f39048d4f03a23da13078a44cdb">[demo]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/archinetai/audio-diffusion-pytorch">[code]<i class="fas fa-external-link-alt"></i></a></td></tr><tr><td align="center">2023</td><td align="center">CVSSP</td><td align="center">AudioLDM</td><td align="center">AudioLDM: Text-to-Audio Generation with Latent Diffusion Models</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2301.12503.pdf">[2301.12503]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://audioldm.github.io/">[demo]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/haoheliu/AudioLDM">[code]<i class="fas fa-external-link-alt"></i></a></td></tr><tr><td align="center">2023</td><td align="center">ByteDance</td><td align="center">Make-An-Audio</td><td align="center">Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2301.12661.pdf">[2301.12661]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://text-to-audio.github.io/">[demo]<i class="fas fa-external-link-alt"></i></a></td><td align="center">-</td></tr><tr><td align="center">2023</td><td align="center">Google</td><td align="center">SingSong</td><td align="center">SingSong: Generating musical accompaniments from singing</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2301.12662.pdf">[2301.12662]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://storage.googleapis.com/sing-song/index.html">[demo]<i class="fas fa-external-link-alt"></i></a></td><td align="center">-</td></tr><tr><td align="center">2023</td><td align="center">ETH</td><td align="center">ArchiSound</td><td align="center">ArchiSound: Audio Generation with Diffusion</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2301.13267.pdf">[2301.13267]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://flavioschneider.notion.site/flavioschneider/Audio-Generation-with-Diffusion-c4f29f39048d4f03a23da13078a44cdb">[demo]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/archinetai/audio-diffusion-pytorch">[code]<i class="fas fa-external-link-alt"></i></a></td></tr><tr><td align="center">2023</td><td align="center">Tencent</td><td align="center">InstructTTS</td><td align="center">InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2301.13662.pdf">[2301.13662]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="http://dongchaoyang.top/InstructTTS/">[demo]<i class="fas fa-external-link-alt"></i></a></td><td align="center">-</td></tr><tr><td align="center">2023</td><td align="center">Sapienza University</td><td align="center">MSDM</td><td align="center">Multi-Source Diffusion Models for Simultaneous Music Generation and Separation</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2302.02257.pdf">[2302.02257]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://gladia-research-group.github.io/multi-source-diffusion-models/">[demo]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/gladia-research-group/multi-source-diffusion-models">[code]<i class="fas fa-external-link-alt"></i></a></td></tr><tr><td align="center">2023</td><td align="center">Google</td><td align="center">SPEAR-TTS</td><td align="center">Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2302.03540.pdf">[2302.03540]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://google-research.github.io/seanet/speartts/examples/">[demo]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/collabora/spear-tts-pytorch">[code]<i class="fas fa-external-link-alt"></i></a></td></tr><tr><td align="center">2023</td><td align="center">Google</td><td align="center">Noise2Music</td><td align="center">Noise2Music: Text-conditioned Music Generation with Diffusion Models</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2302.03917.pdf">[2302.03917]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://google-research.github.io/noise2music/">[demo]<i class="fas fa-external-link-alt"></i></a></td><td align="center">-</td></tr><tr><td align="center">2023</td><td align="center">CMU</td><td align="center">MQTTS</td><td align="center">A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2302.04215.pdf">[2302.04215]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/b04901014/MQTTS">[demo]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/b04901014/MQTTS">[code]<i class="fas fa-external-link-alt"></i></a></td></tr><tr><td align="center">2023</td><td align="center">Baidu</td><td align="center">ERNIE-Music</td><td align="center">ERNIE-Music: Text-to-Waveform Music Generation with Diffusion Models</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2302.04456.pdf">[2302.04456]<i class="fas fa-external-link-alt"></i></a></td><td align="center">-</td><td align="center">-</td></tr><tr><td align="center">2023</td><td align="center">Microsoft</td><td align="center">FoundationTTS</td><td align="center">FoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2303.02939.pdf">[2303.02939]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/cognitivespeech/cognitivespeech.github.io/commit/19ace4c953d77e548ba60e06feeaf4b5a5ce9d2b">[demo]<i class="fas fa-external-link-alt"></i></a></td><td align="center">-</td></tr><tr><td align="center">2023</td><td align="center">Microsoft</td><td align="center">VALL-EX</td><td align="center">Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2303.03926.pdf">[2303.03926]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://vallex-demo.github.io/">[demo]<i class="fas fa-external-link-alt"></i></a></td><td align="center">-</td></tr></tbody></table></div></div></div></div></div><div class="page-main-content-bottom"><footer class="footer"><div class="info-container"><div class="copyright-info info-item">&copy; <span>2023</span> - 2023 &nbsp;<i class="fas fa-heart icon-animate"></i> &nbsp;<a href="/">RevoSpeech</a></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="website-count info-item">总字数&nbsp;25.4k 总访问量&nbsp;<span id="busuanzi_value_site_pv"></span></div><div class="theme-info info-item">由 <a target="_blank" href="https://hexo.io">Hexo</a> 驱动&nbsp;|&nbsp;主题&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.6.1</a></div></div></footer></div></div><div class="right-bottom-side-tools"><div class="side-tools-container"><ul class="side-tools-list"><li class="tools-item tool-font-adjust-plus flex-center"><i class="fas fa-search-plus"></i></li><li class="tools-item tool-font-adjust-minus flex-center"><i class="fas fa-search-minus"></i></li><li class="tools-item tool-dark-light-toggle flex-center"><i class="fas fa-moon"></i></li><li class="tools-item rss flex-center"><a class="flex-center" href="/atom.xml" target="_blank"><i class="fas fa-rss"></i></a></li><li class="tools-item tool-scroll-to-bottom flex-center"><i class="fas fa-arrow-down"></i></li></ul><ul class="exposed-tools-list"><li class="tools-item tool-toggle-show flex-center"><i class="fas fa-cog fa-spin"></i></li><li class="tools-item tool-scroll-to-top flex-center"><i class="arrow-up fas fa-arrow-up"></i> <span class="percent"></span></li></ul></div></div><div class="zoom-in-image-mask"><img class="zoom-in-image"></div><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-input-field-pre"><i class="fas fa-keyboard"></i></span><div class="search-input-container"><input autocomplete="off" autocorrect="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="close-popup-btn"><i class="fas fa-times"></i></span></div><div id="search-result"><div id="no-result"><i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></main><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/utils.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/main.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/header-shrink.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/back2top.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/dark-light-toggle.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/local-search.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/code-block.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/lazyload.js"></script><div class="post-scripts pjax"></div><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/libs/pjax.min.js"></script><script>window.addEventListener("DOMContentLoaded",()=>{window.pjax=new Pjax({selectors:["head title",".page-container",".pjax"],history:!0,debug:!1,cacheBust:!1,timeout:0,analytics:!1,currentUrlFullReload:!1,scrollRestoration:!1}),document.addEventListener("pjax:send",()=>{KEEP.utils.pjaxProgressBarStart()}),document.addEventListener("pjax:complete",()=>{KEEP.utils.pjaxProgressBarEnd(),window.pjax.executeScripts(document.querySelectorAll("script[data-pjax], .pjax script")),KEEP.refresh()})})</script></body></html>