[{"title":"音频生成 | 基于语言建模的 AudioLM","url":"/2023/01/16/audiolm/","content":"音频生成（Audio Generation）是最近非常热门的方向，是AIGC的具体应用之一。相比于语音，音频包含的意义更广泛，不仅包含语音识别/语音合成所针对的人说话声，还包括音乐声、环境声、动物声等各种各样的声音。\n本文介绍 2022 年 9 月份 Google 提出的 AudioLM，将语言建模的思想应用在音频生成任务上，能够生成高质量的音频，并保持音频长时间范围的连续性和一致性。语言建模最近在文本生成、图像生成、视频生成等各类生成式 AI任务中均得到了成功应用，比如 2023 年 1 月微软提出的语音合成模型 VALL-E，同月 Google 提出的图像生成 SOTA 模型 MUSE 也采用了类似的技巧。\n\n\n\n会议/期刊\n年份\n题目\n链接\n\n\n\narxiv\n2022\nAudioLM: a Language Modeling Approach to Audio Generation\nhttps://arxiv.org/abs/2209.03143\n\n\n论文概述本文提出的 AudioLM，能够在长时间范围内保持生成音频的一致性和连贯性，在语音和钢琴声的续写任务上都进行了效果验证。从模型的构成来看，AudioLM 可以说是集大成者：将音频编解码的 SoundStream 模型、自监督表征的 wav2vec-Bert 模型以及强大的 Transformer 语言模型进行了结合。\n\n\nwav2vec-Bert 用于提取粗粒度的语义 token（Coarse Semantic Tokens）表征，在粗粒度语义 token 上自回归建模，能够对音频的局部信息（比如语音的音素、钢琴曲的旋律）和全局信息（比如语音的内容、钢琴曲的和声和节奏）进行有效建模，但是建模粒度比较粗糙，无法保证合成音频的细节质量。\n\n\n\nSoundStream 用于提取细粒度的声学 token（Fine Acoustic Tokens）表征，音频编码时的 token 保留了音频波形细节的信息，通过解码器能够恢复原始波形，实现音频的高质量生成。\n\n\n\n将 wav2vec-Bert 与 SoundStream 两个模型相结合，均当作音频的离散化 tokenizer，再使用 Transformer 语言模型对两类 token 同时建模，从语义内容和声学细节两个角度保证了合成音频的质量。\n\n\n\nAudioLM 使用的 Transformer 语言模型，并不直接在音频波形/采样点级别进行建模（音频的采样点序列太长，使用 Self-Attention 计算复杂度过高），而是在预训练模型抽取的离散 token 上进行建模（对应采样率通常很低），极大降低计算量的同时最大程度保留音频的可恢复性。\nAudioLM 模型模型构成AudioLM 一共包含三个大模块：\n\ntokenizer 模型： 将音频采样点序列（高采样率） 映射为离散 token 序列（低采样率），数学表达式为：离散 token 序列的长度  通常远小于采样点序列的长度 。\n语言模型： 语言模型的输入是离散化后的 token 序列 y，训练时的目标是最大化似然函数LLH（Likelihood），推理时采用自回归的方式预测序列 。\ndetokenizer 模型： 将预测得到的 token 序列  ，还原为原始的音频波形 。\n\n需要说明的是，AudioLM 训练时，tokenizer 和 detokenizer 都是在大量的音频数据集上预训练好的模型，具有很好的泛化性，因此这些参数固定不变，只需训练语言模型部分的参数即可。\n不同的离散表征针对音频生成任务，将输入音频离散化为 token 时，主要有两点要求：一方面，离散的 token 满足较低比特率的同时，能够恢复出高质量的音频；另一方面，离散的 token 能够获取到音频的长时间粒度下的语义表征，使得后续的语言模型能够利用语义信息，保持音频节奏/语义内容等方面的连贯性。然而，基于前人在音频领域的研究，这两个要求的出发点不同，往往是存在矛盾的，因为一般来说更 compact 的语义表征会伴随着波形细节信息的明显丢失。\n针对第一个要求，选用 SoundStream 的出发点是 SoundStream 本身用于音频压缩，目标恰好是在低比特率下也能恢复出高质量原始音频；而针对第二个要求，wav2vec-BERT 则是从语义的角度进行离散特征的建模，符合模型的设想。\n\n\nAcoustic TokensAudioLM 的声学 token 是用 SoundStream 抽取得到的，具体原理详见相关论文笔记链接。对于 16 kHz 的音频，经过编码器降采样 320 倍后采样率变为 16000/320 = 50 Hz，设音频的采样点数为 ，Encoder 输出的 embedding 经过多层 RVQ 进行量化后，得到  的 token 矩阵，其中  表示 RVQ 量化器的个数。矩阵中每个元素都是在 1 到  范围内的整数，其中  代表每个量化器中 codebook 的大小（向量的个数）。\nSemantic TokensAudioLM 的语义 token 是用 wav2vec-BERT 抽取得到的。如下图所示，wav2vec-Bert 由多个 Conformer Block 构成，训练目标结合了两种主流的自监督表征学习方法：wav2vec 的对比学习（Constrastive Learning）目标和 BERT 的 MLM （Masked Language Model）。AudioLM 在 MLM 损失函数对应的中间层输出 embedding 上进行 k-means 聚类，预设  个聚类中心，将每个聚类中心的 index 作为该类别所有向量的离散化语义 token。\n\n\n论文发现，在 k-means 聚类之前，需要将 wav2vec-BERT 中间层输出的 embedding 先进行正则化，使得 embedding 的均值为 0、方差为 1；正则化之后再聚类能够显著提升语义 token 在音素分类等任务上的区分效果。\n由于 wav2vec-BERT 的降采样力度更大，语义 token 的采样率为 25 Hz。对于采样点数为  的 16 kHz 音频，相当于进行了 640 倍的降采样，所以 wav2vec-BERT 离散化的语义 token 序列为 。\n两种 Token 的评测AudioLM 论文先对两种 token 各自的优缺点进行了实验验证与分析，包含两组实验：一组用于衡量 token 的语义能力，评测指标是音素区分任务的错误率；另一组用于衡量 token 的声学特性，对两种不同的 token 分别训练一个 SoundStream 结构的解码器（以最小化重建损失函数为训练目标），评测时对 token 进行重建后评价音频质量，采用客观评价指标 ViSQOL。实验结果如下文表格所示，wav2vec-BERT 得到的 token 在音素区分任务上具有明显更低的错误率，而且错误率随着比特率的提升而降低，表明 wav2vec-BERT 提取的 token 语义能力更强；而 SoundStream 的音频重建效果更好，比 wav2vec-BERT 显著更好，具有更好的声学表征能力。\n\n\n两种 Token 的联合为了充分发挥两种不同 Tokenizer 的优势，AudioLM 提出了层次化建模的方法，先获取整个输入音频的语义 token，再将语义 token 作为模型的条件输入，预测声学 token。\n层次化建模的思想有一个前提假设：给定过去时刻的语义 token（），当前时刻的语义 token（）与过去时刻的声学 token（）之间是条件独立的，近似认为当前语义 token 与声学 token 的历史信息没有关系，数学表达式为：\n\nAudioLM 建模三阶段\n\n上图给出了 AudioLM 建模的三阶段流程，建模粒度从粗到细：从最粗粒度的语义 token，到声学 token 中的粗粒度表征，再到声学 token 中的细粒度表征，层层递进，逐步合成高质量的音频。\n阶段一：语义建模\n\n阶段一用 wav2vec-BERT 抽取音频的离散语义 token，在语义 token 上进行自回归建模 ，用于建模长时间范围内的语义结构。\n阶段二：粗粒度声学建模\n\n阶段二采用和语义建模类似的方法，但只在 SoundStream 的前  个量化器的输出上进行自回归建模，同时将第一阶段得到的语义 token 作为条件输入。由于 SoundStream 的 RVQ 属于多阶段量化，声学 token 也具有一定的层次结构，论文认为：前若干个量化器（粗粒度量化器，coarse quantizer）可以认为主要用来恢复说话人特性、录制环境等偏向全局的粗粒度信息，而剩下的量化器（细粒度量化器，fine quantizer）则更侧重于波形的细节信息。\n阶段二主要建模 ，其中：\n\n 表示第一阶段得到的离散语义 token 序列；\n 表示之前时刻，前  个粗粒度量化器的声学 token 序列，并且将矩阵展开（flatten）；\n 表示当前时刻下，前  个量化器的 token 序列，同样进行展开。\n\n阶段三：细粒度声学建模\n\n阶段三在细粒度量化器输出的声学 token 上进行建模，使用  个粗粒度量化器的 token 作为条件输入，对  进行建模，其中：\n\n 表示前  个量化器输出的 token 序列，将矩阵展开；\n 表示之前时刻，第  到第  个量化器（一共  个量化器）输出的 token 序列，并且展开；\n 表示当前时刻下，前  个量化器的 token 序列，并且展开。\n\n实际上， 和  相当于  时刻之前，全部  个量化器输出的 token 序列在时间维度上进行了展开，以及第  时刻将第  个量化器之前输出的 token 作为条件输入。阶段三没有考虑第一阶段的语义 token 序列，是增加了条件独立假设：给定粗粒度量化器的 token 时，认为细粒度量化器输出的声学 token 与第一阶段的语义 token 是条件独立的。\nAudioLM 的推理\n非条件式音频生成： 第一种推理方式，模型没有任何外部条件输入，直接通过随机采样得到语义 token，然后作为粗粒度声学建模的条件输入，其输出再作为细粒度声学建模的条件输入，经过 detokenizer 即可得到生成的音频。虽然是非条件式生成，但是后续实验验证了生成音频整体的一致性。\n声学生成： 第二种推理方式，模型以真实的音频作为条件输入，wav2vec-BERT 基于真实音频得到的语义 token，再通过第二三阶段的处理，经过 detokenizer 生成得到音频。这种推理方式下，所生成音频的语义信息是来自于真实音频的，只不过经过了模型的后续处理变得更多样，但是语义内容没有发生变化。\n音频续写生成： 第三种推理方式，给定一段音频的前缀或者 prompt，然后生成后续的音频。对于给定 prompt：\n先抽取 prompt 对应的语义 token 序列和粗粒度量化器的 token 序列；\n第一阶段：利用 prompt 真实音频的语义 token 序列，预测之后的语义 token 序列；\n第二阶段：已有 prompt 真实音频对应的语义 token 序列、第一阶段预测出的后续语义 token 序列，以及根据 prompt 真实音频提取的粗粒度声学 token 序列；将三个序列拼接，预测出后续的粗粒度声学 token 序列；\n第三阶段：将第二阶段获取到的完整的粗粒度声学 token 序列作为细粒度模型的输入，预测得到完整的细粒度声学 token 序列；\n最后，基于粗粒度和细粒度的声学 token 序列，使用 detokenizer（SoundStream 的解码器）生成音频。\n\n\n\n实验与分析实验设计与准备两种实验任务: 分别是语音续写和钢琴曲续写：语音续写需要生成的音频满足音色、录音环境、韵律方面的一致性，并且语义上保证准确和连贯；钢琴曲续写则需要生成的音频满足旋律、和声和节奏上的连续性。语音续写是基于 3 秒的语音 prompt 继续生成 7 秒的音频；钢琴曲续写是基于 4 秒的音频 prompt 继续生成 20 秒的音频。\n数据集： 语音数据采用无标签的 6 万小时的 Libri-Light 数据；钢琴曲使用的是谷歌自有的 4 万小时数据，覆盖初学者到钢琴家级别、不同声学环境、不同曲调的钢琴曲声音。\n实验配置： wav2vec-Bert 采用 6 亿参数量的模型，输出 embedding 来自 MLM 部分的第 7 层，离散化聚类时类别个数 ；SoundStream 采用 320 倍降采样， 个量化器，前 个量化器作为第二阶段的粗粒度声学 token，后面 8 个量化器用于第三阶段的细粒度声学 token；而 Transformer 自回归语言模型的参数：12 层，self-attention 包含 16 个 head、embedding 维度为 1024，feed-forward 层的隐层大小为 4096，使用的是  相对位置编码。\n语义 token 的信息本实验用于测试语义 token 是否保留了语音的内容信息。使用 LibriSpeech test-clean 中 4-10 秒内的音频，采用第二类推理方式声学生成，每条音频随机生成三个样本，使用 Conformer-Transducer 的 Large 模型对生成的样本进行识别，计算得到 CER 和 WER，用于衡量生成音频在语义内容上的正确性和一致性。\n从下表的实验结果可以看出，AudioLM 生成的语音在内容上的一致性还是相对比较高的，尤其是考虑到声学生成任务中，新生成的音频可能会带有一些额外的噪声，导致语音识别系统的效果也会有些降低。\n\n\n声学 token 的信息同样地，本部分用于验证声学 token 包含的声学特性的信息。使用 LibriSpeech 中的 291 个说话人的音频，在第二种（声学生成）和第三种（音频续写）推理方式下，对新生成的音频和原始音频进行说话人分类，实验结果如下图所示。从中可以得到一些结论：声学生成因为只保留了真实音频的语义 token，不包含说话人音色方面的信息，所以说话人分类准确率很低；但是音频续写以 prompt 的真实声学 token 作为部分条件输入，保持了音色的连续性，因此说话人分类准确率很高。\n\n\n语言学信息评测论文还针对基于语义 token 进行语言建模的方法进行更细致的评价，采用以下两种评价指标（均来自于 ZeroResource 2021 年的比赛），验证 AudioLM 在语义内容上的建模效果：\n\nsWUGGY：两个发音相近的词，一个真实存在，一个并不是真正的词，如果能够给真实存在的词更高的概率，说明模型效果越好；\nsBLIMP：模型需要赋予语法正确的句子比语法错误的句子更高的概率，准确率越高，说明模型的语义能力越强。\n\n下图是两个评测的实验结果，可以看出，AudioLM 的语义建模能力很强。\n\n\n\n实验细节说明：实验评测使用 ZeroResource 2021 年比赛的开发集，针对 sWUGGY 和 sBLIMP 两个评测，分别有 10000 和 6300 组测试数据对。sWUGGY 评测时，只考虑在 LibriSpeech 中出现过的词（集内词）。使用模型输出的似然值作为评价概率高低的依据，但是 sBLIMP 评测的正样本和负样本序列长度明显不同，为了避免句子长度对评测的影响，将似然值除以序列长度，正则化之后作为判断句子正确与否的指标。AudioLM 力压包括 BERT, HuBert, RoBERTA, CPC_BERT 等在内的一众模型，达到了最优的效果。\n\n钢琴曲续写实验针对钢琴曲的续写，论文单独使用 4 万小时钢琴声训练了一个 AudioLM 模型，不过对 SoundStream 模块的参数进行了更改，采用 3 层的 RVQ，每层 codebook 的大小为 ，所以去除了第三阶段的训练，而是在第二阶段一次性预测所有的声学 token。实验组音频是使用 AudioLM 进行续写，对照组音频是去除了 wav2vec-BERT 提取的语义 token 建模，主观评测显示 83.3% 评测数据对中，AudioLM 续写的钢琴曲更受评测人青睐。\n参考文献/链接\nAudioLM 示例音频：https://google-research.github.io/seanet/audiolm/examples\nAudioLM 官方博客：https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html.\nSoundStream：Zeghidour, Neil, et al. “Soundstream: An end-to-end neural audio codec.” IEEE/ACM Transactions on Audio, Speech, and Language Processing 30 (2021): 495-507. [pdf]\nSoundStream 论文笔记：https://revospeech.github.io/2023/01/14/lyra_v2_soundstream.\nwav2vec-BERT：Chung, Yu-An, et al. “W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training.” 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2021. [pdf]\nZeroChallenge 2021 评测指标：https://zerospeech.com/challenge_archive/2021/02_track1/#evaluation\n\ndemo 视频","categories":["论文笔记"],"tags":["音频生成","语音合成"]},{"title":"生成模型基础 | GAN 的基础（一）","url":"/2023/01/21/gan_basics/","content":"生成对抗网络（GAN，Generative Adversarial Nets）是 Yoshua Bengio 团队在 2014 年提出的，一作是 Ian Goodfellow。Yoshua Bengio 团队坚持认为：深度学习的目标是发现更丰富、更加层次化的模型，能够用来表示各种数据的概率分布。2014 年时，深度学习方法在判别式模型方面已经有很多突出的成果，用于将高维输入特征映射到不同的类别标签，基于梯度的反向传播方法优化模型。然而当时深度学习在生成式模型方面的影响性成果较少，主要是因为：当时提出的生成模型在最大似然估计（MLE，Maximum Likelihood Estimation）的过程中需要近似很多难处理的概率运算（近似推断之类），而 GAN 正是为了规避之前生成模型的缺点而提出的一种新型的生成模型。\n在智能语音领域，GAN 的应用非常广泛，在语音合成、语音降噪等生成式任务中是一类重要的方法。先在本文介绍下 GAN 的基础知识，包括 GAN 模型的基本思想、数学分析、优化方法及训练目标等，后续会针对语音合成的声码器等具体任务介绍研究者提出的各种模型。本系列主要涉及下面的三篇论文，本文介绍第一篇GAN 的提出。\n\n\n\n会议/期刊\n年份\n题目\n链接\n\n\n\nNeurIPS\n2014\nGenerative Adversarial Nets\n[pdf]\n\n\nNeurIPS\n2016\nImproved Techniques for Training GANs\n[pdf]\n\n\nICCV\n2017\nLeast Squares Generative Adversarial Networks\n[pdf]\n\n\nGAN（生成对抗网络）包含生成器（Generator，生成式模型）和判别器（Discriminator，判别式模型）两大模块，GAN 的核心思想正是在于生成器和判别器的对抗（Adversarial）。在实际应用中，通常只用到生成器部分，用于生成图像、语音或音频等，但实际上训练出效果好的生成器离不开判别器，判别器的作用是区分样本究竟是从生成模型生成的假样本还是真实样本。GAN 实际上提出了一套新的框架，和具体的训练算法、模型结构和优化算法都无关。\n对于生成器和判别器间存在的对抗关系，论文给出一个形象的比喻：生成器相当于造假币的人，而判别器则相当于警察；所谓的对抗关系是指，造假币的人会为了让警察无法区分货币的真假而提升造假币的能力，同时警察也会提高自己区分真假币的能力（判别器的同步优化），优化时生成和判别两个模型的能力都会提升。对于最终训练完成的生成器，理想的状态是判别器无法区分样本究竟是真实的还是生成的，从而达到“以假乱真”的效果。\nGAN 之前的生成模型在 GAN 被提出之前，大多数生成模型的工作集中于已经给定具体概率分布函数 pdf 的情况。已知概率分布函数时可以通过最大对数似然目标来训练，比较典型的有效方法是深度玻尔兹曼机（DBM，Deep Boltzman Machine），但实际上生成模型处理的似然函数都非常复杂，往往需要对似然函数的梯度进行大量近似运算。\n为了避免上述大量梯度近似的问题，有研究者提出生成式机器（Generative Machine），不需要表征出具体的似然函数形式，也可以生成所需概率分布的样本，典型的工作是生成式随机网络，可以直接用反向传播来训练，但生成随机网络一般依赖于马尔科夫链，GAN 消除了这一依赖并进行了拓展。\n在 GAN 工作的同时，有其他研究者延续了生成随机网络的思想，提出了更通用的随机反向传播方法，能够对方差有限的高斯分布进行反向传播，更新均值和协方差的参数，用于变分自编码器（VAE，Variational AutoEncoder）的训练。VAE 是另一种生成式模型，之后在另外的文章中会详细解读。从思想上来看，VAE 和 GAN 都是使用两个网络协同训练，但是 VAE 中的第二个网络作为判别模型时进行了近似推断，此外 GAN 不能够建模离散的数据，VAE 不能使用离散的隐变量。\n之前有类似于 GAN 的采用判别式模型协助训练生成模型的工作。PM（Predictability Minimization）的作者 Jürgen Schmidhuber 曾经和 GAN 的作者 Ian Goodfellow 产生过争议（YouTube 链接），PM 的思路是第一个网络的每个隐层单元都朝着和第二个网络当前输出结果不同的方向去训练，思想上是有几分相似的地方。Ian Goodfellow 阐明了 GAN 和 PM 的三点不同之处：\n\nGAN 的生成器和判别器之间的对抗训练，是模型训练的唯一目标，该目标已经足够训练网络；但是 PM 严格来说相当于辅助隐层单元训练的正则项，不是训练的主要目标；\n在 PM 中，两个网络的输出之间进行对比，其中一个网络的目标使得两者的输出更加接近，另一个网络的目标是使得两者的输出不同；但是 GAN 没有对比两个网络的输出，而是将第一个网络得到的高维特征信息作为第二个网络的输入，目标是让第二个网络对输入丧失真假的判别能力；\nPM 的训练目标是单纯的优化问题，最小化定义的目标函数；但是 GAN 包含了两层目标的同步优化，即博弈中的 Minimax，在两层优化目标中间找到一个相对的平衡点。\n\nGAN 的数学含义原始的论文中为了简化问题，生成器和判别器都使用普通的多层感知机（MLP，Multi-Layer Perceptron），将生成模型的输入设定为随机噪声。\nGAN 的生成器目的是从数据  中学习分布 ， 表示 generator。GAN 对问题的形式做了个变换，引入噪声变量 ，先定义  的先验分布为 ，GAN 的生成器是将输入的噪声  映射到数据 ，这一过程用  来表示， 是作为输入的噪声变量， 表示生成器 MLP 的参数。\nGAN 的判别器可以定义成  ，其中  表示判别器的输入， 表示判别器 MLP 的参数。 的输出是一个标量，用来表示输入  来自真实样本（而不是生成器生成）的概率。\n确立了 GAN 的生成器和判别器之后，两者各自的训练目标分别为：\n\n生成器：最小化“生成样本被判别为 0 的概率”的损失函数\n判别器：最大化“表示真假分类准确性的损失函数”，希望将生成的样本正确分类为 0（0 表示假的），将真实的样本正确分类为 1（1 表示真的）\n\n\n一方面，判别器对应于判别任务，假设真实样本  被分类为 1 的概率为 ，生成样本被分类为 1 的概率为 ，在整个数据集上求期望（平均）后，交叉熵损失函数为：另一方面，生成器需要最小化生成样本被判别为 0 的“概率”，可以理解成最大化生成样本被判别为 1 的“概率”，同样也是在整个数据集上求期望，所以交叉熵损失函数为：从生成器和判别器的损失函数来看，核心的博弈是  部分，在生成器和判别器中，这部分的优化目标是矛盾的：判别器中的优化方向是最大化（让判别器将生成的样本分类为假类），生成器中的优化方向是最小化（让判别器将生成的样本分类为真类），这种矛盾的优化目标决定了 GAN 的两层目标的同步优化，在训练过程中体现为判别器和生成器交替训练。将生成器和判别器的优化目标联合起来即为 GAN 的整体优化目标：\n\n其实 GAN 的这一优化目标可能无法让 G 学习的足够好，比如在学习的起始阶段，G 的生成能力很差，和真实样本之间存在明显的差别，因此判别器 D 会高置信地将生成样本判别为类别 0（假样本），这种情况下 对应的梯度为 0 导致无法训练。论文为了解决这一问题，在训练的起始阶段，要求 G 的训练目标不是最小化  ，而是最大化 ，两者目标上是一致的。\nGAN 原理的直观表示\n\n上图直观地表示出 GAN 的训练原理，其中  表示噪声分布， 表示数据分布， 到  带有箭头的过程，表示噪声输入经过生成器后的输出。\n\n图(a)：GAN 的初始阶段，生成器输出的数据分布和真实的样本分布存在较大差异，此时判别器还没更新，因此对两个分布之间的判别能力也比较差；\n图(b)：先优化判别器，生成器的参数固定不变，上述两个分布固定的情况下，能够学出效果更好的判别器；\n图(c)：再优化生成器，判别器的参数固定不变，让生成的样本被判别为真实样本的概率更大，图中体现为生成器输出的数据分布和真实样本数据分布更加接近；\n图(d)：回过头继续优化判别器，生成器的参数固定不变，根据生成样本的新分布，更新判别器在该情况下的判别效果；…\n\n生成器和判别器如此交替更新，直到达到类似图 (d) 所示的理想情况，生成样本分布和真实样本分布完全一样，判别器也完全无法区分样本究竟来自哪个分布，意味着生成的样本已经达到了“以假乱真”的地步。\nGAN 的具体训练流程\n\n对于每次训练迭代：\n\n先执行 k (k 是超参数) 步，固定生成器，更新判别器：\n\n\n从噪声的先验分布  中采样 m 个噪声样本，通过生成器得到对应的 \n从真实数据分布中  中采样 m 个真实样本\n优化交叉熵损失函数 ，更新判别器的参数\n\n\n再执行 1 步，固定判别器，更新生成器：\n\n\n从噪声先验分布中采样 m 个噪声样本\n 作为目标更新生成器参数以上步骤迭代重复很多次，即为 GAN 的一般训练过程\n\nGAN 原理的数学分析GAN 的生成器的目标，是让生成的样本分布和真实样本的分布完全相同。设生成器从数据中训练学习到的分布为 ，真实的理论上的样本分布为 ，GAN 的数学原理是：\n\nGAN 在经过 MiniMax 博弈之后，有个全局的最优点，\n\n\n固定生成器 G 时，求判别器 D 的最优解：\n\n对于优化目标 ，只关注内层的  目标时，目标可以表示为：\n\n其中，第二个等号只是进行了  的替换，对  积分，但是  和  只是符号表示，所以可以合并积分项。目标函数求导后值为 0 时，求解得到 D 的最优解是：\n\n\n将 D 的最优解代回，得到固定 D 优化 G 时的目标函数：\n\n注意，上述公式中  和  都是随机变量的概率分布： 表示真实样本的分布， 表示生成样本的分布。\n\n数学基础知识复习\n\n若连续随机变量 , 则  的期望计算方式如下：\nKL (Kullback-Leibler) 散度 常被用于衡量两个分布（如  和 ）间的相似程度，数学表达式为：计算 KL 散度时对两个分布的顺序是有要求的，属于非对称关系，即 。\nJS (Jensen-Shannon) 散度 也可用于衡量两个分布间的相似程度，解决了 KL 散度的非对称的问题，数学表达式为：JS 散度的计算具有对称性，即 。\n不论是 KL 散度还是 JS 散度，数值都具有非负性。KL 散度的非负性也被称为吉布斯不等式，KL 散度值为 0 当且仅当  时得到。JS 散度非负性很容易从 KL 散度的非负性推导出。\n\n\n回过头来看  的表达式可以变换为：\n因此， 当且仅当  时得到，这也证明出了 GAN 优化目标的全局最优结果是生成器的分布和训练数据的分布完全相同。\nGAN 原论文中的实验主要是图像生成任务，此处不再过多展开，具体可以参考本文结尾的李沐老师的讲解视频。GAN 被提出后，在代码实现和实际训练过程中也暴露出一些问题，后续将具体介绍一些 GAN 中常用的方法（trick）作为补充。\n参考讲解视频\n","categories":["论文笔记"],"tags":["语音合成","生成模型","声码器"]},{"title":"音频编解码 | SoundStream","url":"/2023/01/14/lyra_v2_soundstream/","content":"SoundStream 是谷歌 2021 年提出的一种神经网络音频编解码器，能够在传统编解码器通常使用的比特率下，高效地压缩语音、音乐等各类音频，SoundStream 在音频压缩、音频降噪及音频生成（比如 Google 2022 年 9 月提出的 AudioLM 和 2023 年 1 月提出的 MusicLM）等任务中都有所应用。本文对 SoundStream 的原始论文进行分析和解读。\n\n\n\n\n\n会议/期刊\n年份\n题目\n链接\n\n\n\nTASLP\n2021\nSoundStream: An End-to-EndNeural Audio Codec\nhttps://arxiv.org/abs/2107.03312\n\n\n工作概述四点概括 SoundStream 模型的主要工作：\n\n模型由全卷积 Encoder-Decoder 和残差向量量化（RVQ, Residual Vector Quantizer）模块端到端联合训练得到；\n模型结合了语音合成和语音增强领域的前沿工作，包括对抗训练和重建损失目标等，能够让模型从量化后的编码结果恢复出高质量的音频；\n训练时在量化层使用了结构化 dropout，使得单一模型能够在 3kbps 到 18kbps 不同的比特率下有效使用，相比于固定比特率的模型，音频质量的损失几乎可以忽略不计；\n模型支持将音频压缩编码与音频降噪进行联合建模，达到与级联模型相近的效果。\n\n\n\n\n在实际应用场景中，SoundStream 可修改为低时延的设计，支持流式的编解码推理，在智能手机 CPU 上可达到实时的效果。在主观评测中，对于 24kHz 采样率下的音频，3 kbps 低比特率下的 SoundStream 比 12 kbps 的 Opus 和 9.6 kbps 的 EVS（增强语音服务，Enhance Voice Services）效果都更好。另外，SoundStream 的 Encoder 端或者 Decoder 端允许对压缩编码和语音增强进行联合建模，单一模型的实现，不会额外增加时延。\n\n音频编解码中的比特率比特率，也常被称为码率，用来表示每秒传送的比特（bit）数，单位为 bps（bit per second）。在音频编解码中，比特率是单位时间播放的压缩音频的比特数量。一般来说，同一个音频编码时的比特率高，编码后的文件就越大，因此比特率可以表示音频压缩的力度，比特率越低，音频压缩得越厉害。比特率/码率（kbps）= 文件大小（KB）* 8 / 时间（秒）举例：24 kHz、16-bit 的 PCM 比特率是 384 kbps。比如 2 秒的 24 kHz、16-bit 的 PCM 音频，文件大小为：24 k * 2 秒 * 16 bit / 8 = 96 KB，比特率为 96 KB * 8 / 2 秒 = 384 kbps。关于传统音频编解码（mp3/aac/opus）的比特率，后续会专门补充介绍。此处需要说明的是，同一种音频编解码方法，往往可以通过调整配置达到不同的比特率，从而适应不同场景的需要。\n\n下图中横坐标代表不同编码方法使用的比特率，纵坐标（MUSHRA score）表示主观评测分数，分数越高代表解码出的音频质量越好。从下图可以看出，SoundStream 比之前的音频编解码方案都更好，在 3kps 的低比特率下，音频质量显著优于同比特率下的 Lyra、Opus 和 EVS，能够达到与 9.6 kbps EVS 和 12 kbps Opus 相近的效果。音频质量相近的情况下，比特率只需原来的四分之一到三分之一。\n\nMUSHRA 分数和语音合成领域常用的 MOS (Mean Opinion Score) 分数具有类似的作用，都用来体现主观听感上音频的质量。MUSHRA 分数的优点是：能够基于更少的测试者得到统计意义上显著的结果，具体可见：https://en.wikipedia.org/wiki/MUSHRA\n\n\n\n\n\n音频编解码背景传统的音频编（解）码方法可以分为两大类：波形编（解）码和参数编（解）码。\n波形编码（非参数编码）波形编码的目标是：解码重建的音频信号在波形上维持原音频信号的波形形状，准确地重建出编码前的采样点。波形编码端通常给定一个可逆的编码方式，将输入的时域波形转换到频域，再将频域的系数进行量化和熵编码；解码端则实现逆过程，从量化编码后的频域恢复出原始的时域波形。其中，量化的过程通常是基于感知模型进行比特分配（bit allocation），基于人耳对不同频率成分的感知程度不一样，给不同的频带分配不同的量化位数。\n从上述过程可以看出，波形编码基本没有对音频的内容（语音/音乐/噪声/混响）信息进行先验假设，而是将音频信号作为一般的波形信号来处理，所以是通用的音频编解码方式。波形编解码器通常需要较高的比特率（16-64 kbps）才能恢复出很高质量的音频，在低比特率下会有较明显的量化损失。\n声码化编码（参数编码）针对波形编码的劣势，声码化编码（也称参数编码或者模型编码）对音频（通常是语音）进行了一些特定的假设，使用参数化模型引入了很强的先验信息。\n编码器用来估计模型的参数，再进行量化；解码器使用合成模型从量化后的参数还原出时域的波形。参数编码不强调采样点级别的重建损失，目标是重建后在感知上能够和原始音频相似即可，保持原音频的语义，但重建的波形可能同原始波形有较大的差别。\n参数编码器的优点是比特率低，但解码合成的音频品质较差，自然度低。该类编码器对环境信噪比也比较敏感，在安静的坏境才能给出较高的清晰度，对信道误码也比较敏感。\n综上，以上两种传统的音频编解码需要信号处理流程和工程设计，提升编码的效果需要利用心理声学和语音合成领域的知识。目前产业界常用的 Opus / EVS（增强语音服务，Enhance Voice Services）都算是传统的音频编码方式，已经基本能够在低时延和实时通信的前提下，保证在不同的内容（语音/音乐）、不同比特率、不同采样率下的高效性。\n\n心理声学（psycho-acoustics）是面向人感知的声学，研究声音和它引起的听觉之间的关系，探究“人脑感知和解释声音的方式”。\n\n神经网络音频编解码近些年，基于机器学习和深度学习的方法已经应用于音频编解码方向，主要分为以下两类思想：\n\n作为音频解码器的后处理模块：比如，在现有音频解码器恢复出的音频基础上，使用音频超分（super-resolution）的方法扩展频带；或者使用音频降噪的思想去除编码损失带来的失真；或者使用丢包补偿的方法。\n\n集成到音频编解码器中：语音合成中的神经网络声码器，可以自然地应用于音频编解码（同样是音频波形压缩到某一种低维特征，再从低维特征恢复生成原始的音频信息），不同的工作主要体现在模型结构的差异：WaveNet 使用的是全卷积结构，LPCNet 使用的是 WaveRNN，而 Google 2021 年 2 月提出的 Lyra 使用的是 WaveGRU，以上这些工作主要针对的都是低比特率的音频编解码场景。\n\n\nSoundStream 是 Google 继 Lyra 之后提出的另一种神经网络音频编解码器，属于上面的第二类。\n\nSoundStream 模型结构\n\n\nSoundStream 编解码器是全卷积的结构。输入是原始的音频波形，Encoder 将其映射为较低采样率的 embedding 序列，RVQ 残差向量量化器对 embedding 序列进行量化；Decoder 同样是全卷积结构，输入是量化后的 embedding，预测目标是恢复出原始波形。\nSoundStream 模型是基于波形重建和对抗训练两个损失函数进行端到端训练的，增加了多个判别器用于区分是解码恢复的音频还是原始音频。需要说明的是，Encoder 和 Decoder 都只使用了因果卷积，不依赖于音频后续采样点的信息，所以模型时延只与编码器的降采样系数有关。具体计算过程为：假设音频的原始采样率是 24 kHz，降采样 320 倍到 75 Hz，那么模型的时延为 1 / 75 ≈ 13.3 ms，因为需要等原始音频输入 320 个新的采样点（320 / 24000 ≈ 13.3 ms）编码器才能输出一个新的 embedding。\n编码器结构\n\n\n\n编码器的输入是 24 kHz 原始波形，先进入一层一维卷积，kernel_size 为 7，输出 channel 大小为  ；再经过  个 EncoderBlock 模块，每个模块包含三个 ResidualUnit 残差单元和一层用于降采样的一维卷积。\nResidualUnit包含两层一维卷积：第一层是膨胀卷积， kernel 大小为 7，输出 channel 为 N，膨胀率为 dilation（用于扩大深层网络的感受野）；第二层是输出 channel 为 N，kernel size 为 1 的一维卷积（相当于全连接层）。\nEncoderBlock包含的三个膨胀率分别为 1，3，9 的残差单元，膨胀率越大说明卷积层的感受野越大；三层膨胀卷积之后是一层跳步卷积（strided convolution），stride=S 表示对输入序列进行 S 倍的降采样。\n按照上图给出的网络结构示例，共四层 EncoderBlock，降采样倍数分别为 2, 4, 5, 8，相当于整个编码器的整体降采样倍数为 320，对应于输入的 24 kHz 音频，输出帧率为 24000/320 = 75 Hz。此外，每个 EncoderBlock 在输出的 channel 维度上是输入 channel 的 2 倍，四层 EncoderBlock 之后输出的 channel 维度从  扩充至 。四层 EncoderBlock 之后是一层 kernel_size 为 3 的一维卷积，输出 channel 维度为 K，即对应于最终 embedding 的维度。\n其他细节为了保证模型的实时性和低时延，模型中用到的所有一维卷积全部采用因果卷积，卷积计算只会用到当前及之前的信息，padding 的操作也只应用于过去的序列。另外，所有的卷积层只使用 ELU 激活函数，不加入任何形式的 normalization 层。\n解码器结构\n\n\n解码器采用的是和编码器完全对偶的结构。编码器得到的 embedding 经过一维卷积后进入  个 DecoderBlock 模块。每个 DecoderBlock 先进入一层一维反卷积进行上采样，再经过三层残差单元将输出 channel 减半，三层残差单元的膨胀卷积率仍然是 1, 3, 9 的顺序。 层 DecoderBlock 之后是一层输出 channel 为 1 的一维卷积，相当于将当前时刻的输出映射到原始的时域波形上。\n残差向量量化器 (RVQ)SoundStream 整体的编解码器结构比较直观，但论文的关键技术点之一是引入了残差向量量化（RVQ）模块，目的是将 Encoder 输出的 embedding 通过量化层压缩到目标的比特率。\n先回顾下 VQ（Vector Quantization）：VQ 的目标是学习 N 个向量构成的 codebook，用于对 Encoder 输出的 embedding 进行编码。设 Encoder 输出的 embedding 序列长度为 S，每个 embedding 的维度为 D，使用 VQ 进行编码后，每个 embedding 被重新映射为一个 one-shot 向量，向量中 1 的位置用于表征对应 codebook N 个向量中的哪个，因此量化后对应的序列为 S × N，N 作为 one-hot 向量可以用  比特来存储。\n\ncodebook 也称码本，实际上一个向量组，在更早的论文中也可以称为字典 dictionary，用于表明这些向量是最基本的向量表征，在使用时，N 个向量只需要  比特用于指明是哪个向量即可。\n\n普通 VQ 的局限性计算下 VQ 所需的 codebook 大小：如果目标比特率是 6 kbps，对于 24 kHz 的音频，按照前文图中的 320 倍降采样，每秒对应于 75 个 embedding，每个 embedding 对应的比特数为 6000 / 75 = 80 bit，那么对应的 codebook 大小是 ，这个量级肯定是行不通的，因此普通版本的 VQ 因为 codebook 过大而不适用。\n\n关于 VQ 中一些更深入的概念，包括 Straight-Through Estimator，commitment loss 等，详见：Encodec 的论文讲解。\n\n残差 VQ / 多阶段 VQ为了解决普通 VQ 方法中 codebook 规模过大的问题，SoundStream 采用多阶段 VQ 的方法。RVQ 量化器一共包含层 VQ，基本流程如 Algorithm 1 所示（ 表示第 i 层量化层）：原始的 Encoder 的输出的 embedding 经过第一层 VQ，可以计算出相应的量化残差，然后第二层 VQ 只针对上一层 VQ 的残差进行量化，以此类推。\n值得注意的是，论文将原本的一层 VQ 转换为多层残差 VQ 时，每个 VQ 层的 codebook 大小保持一致，相当于比特率的降低是按照倍数平均分配到每个 VQ 层的。按照前文 24 kHz 音频压缩到 6 kbps 的例子：当使用的 VQ 层共时，每个 VQ 对应的 codebook 大小可以变为，此时就是一个相对可行的 codebook 大小了。\n\n\n\ncodebook EMA 训练每个量化器在训练 codebook 的时候，都使用 EMA (Exponential Moving Average，指数移动平均)的更新方式。训练 VQ 的 codebook 使用 EMA 方法由 Aäron van den Oord首次提出。论文 Neural Discrete Representation Learning（https://arxiv.org/abs/1711.00937）提出使用 EMA 指数移动平均的方式训练码本 codebook。\n假设可以一次性获取训练集对应于 Encoder 的所有输出，设 codebook 上一次迭代后其中某个向量为，那么本次迭代只需求出 Encoder 输出中和距离最近的向量，取平均值即可作为本次迭代后的数值。这实际上和 k-means 中聚类中心的迭代方式一样，但这种思想没有办法应用于 mini-batch 级的数据，因为每个 batch 只包含全部训练集的很小一部分，基于 mini-batch 的统计和平均是有偏的，因此需要采用一种随着 mini-batch 的变化在线更新 codebook 的方法。\n用表示 codebook 第 i 个向量在第 t 次迭代时与其距离最近的数据点（本文中即 Encoder 输出的 embedding）累计个数，表示这些数据点的 embedding 之和，即可用和计算得到在第 t 次迭代后的数值：\n\n指数移动平均主要体现在和的计算方式上：假设已经知道第 t-1 次迭代后的和，若第 t 次迭代新增了个与最近的 embedding，这个 embedding 用来表示，那么第 t 次迭代时和分别按照下列式子计算：\n\n\n从计算方式可以看出，每次迭代相当于对之前所有 batch 累计值和当前 batch 新获取的数据值进行加权平均，权重又称为 decay factor，通常选择数值为 0.99 ，使得参数的迭代更新不至于太激进。\ncodebook 初始化及更新SoundStream 在初始化 codebook 的各个向量时，对第一个 batch 数据 Encoder 输出的 embedding 进行 k-means 聚类，使用聚类中心作为各向量的初始值，使得 codebook 在开始训练时和输入的数据分布保持相近。\n如果 codebook 中某个向量在多个 batch（可以对具体的 batch 数进行预设）都没有可用的 embedding 来更新参数，该向量会使用当前 batch 内随机一个 embedding 进行覆盖。这个思想是参考了 JukeBox（https://arxiv.org/pdf/2005.00341.pdf）论文中的做法，是为了让 codebook 中的向量在训练时被及时的使用，因为 codebook 中的向量只用被用到了才能从损失函数得到反馈进行反向传播的参数更新，从而规避 codebook 的无效学习。\n灵活的比特率按照前文的描述，RVQ 的层数和每个 RVQ 的 codebook 大小确定时，音频压缩后的比特率也是固定的，这就要求对不同比特率的应用场景分别训练不同配置的模型。但是 SoundStream 利用了 RVQ 的残差连接的优势，使得所有的 RVQ 层不一定需要全部使用，训练时可以对 RVQ 层进行结构上的 Dropout，从而给出 SoundStream 的另一大优势：很灵活地适配不同的比特率。具体操作方法为：设 RVQ 的层数为，对于每个训练样本，随机从 1 到中选择一个数，对应于不同的比特率，训练阶段只需要经过前个 RVQ 层；推理阶段也可以根据不同比特率的需要，使用相应的前个 RVQ 模块进行预测。\n判别器SoundStream 为了提高编解码之后音频的合成质量，将语音合成中常用的对抗训练思想引入到模型中，额外增加了两种判别器，用来判别音频是编解码恢复出来的还是真实的音频。\n第一种是基于波形的判别器。采用多精度 multi-resolution 的思想，与 MelGAN 和 HiFi-GAN 中的多精度判别器类似，在原始波形、2 倍降采样和 4 倍降采样后的波形上分别进行真假判别。\n第二种是基于 STFT 的判别器：\n\n输入的 24 kHz 原始波形先进行 STFT 短时傅里叶变换，使用的窗长（window length） = 1024 个采样点，跳步（hop length） = 256 个采样点。经过 STFT 操作后得到二维的时频域输出（， 为时域采样点个数， 表示 STFT 后的 frequecy bin 个数，由选用的窗长  决定，）\n然后输入到一层 kernel size 为 7 × 7、输出 channel 个数为 32 的二维卷积中，之后是若干层 ResidualUnit，结构上是将编解码器中 ResidualUnit 中的一维卷积全部替换为二维卷积\n每个 ResiduaUnit 内部包含两层二维卷积，第一层 kernel size 为 3 × 3，第二层在不同 ResidualUnit 内是不同的：二维卷积的 stride 是 (1, 2) 和 (2, 2) 两组参数交替使用，图中和 分别表示时域和频域的 stride，代表两个维度的降采样倍数；根据 stride 参数的不同，分别对应于 (3, 4) 和 (4, 4) 的 kernel size，使得 stride 较小时感受野不必过大。\n图示的 6 层 ResidualUnit 中，第一层二维卷积的输出维度是 C, 2C, 4C, 4C, 8C, 8C 的变化规律，而第二层二维卷积的输出维度是 2C, 4C, 4C, 8C, 8C, 16C 的变化规律。6 层 ResidualUnit 之后，时域降采样倍数为 1×2×1×2×1×2=8，频域降采样倍数为 2×2×2×2×2×2=64，二维输出的大小为 (, F/64)。\n最后使用全连接将其映射为单个数值的 logits，表示该波形是编解码后恢复的还是真实的音频。\n\n\n\n\n\n训练目标SoundStream 整体使用 GAN（生成对抗网络）作为训练目标，采用 hinge loss 形式的对抗 loss。对应到 GAN 模型中，整个编解码器作为 Generator 生成器，使用前文所述的两种 Discriminator 判别器：一个 STFT 判别器和三个参数不同的 multi-resolution 判别器。判别器用来区分是解码出的音频还是真实的原始音频，本文采用 hinge loss 形式的损失函数进行真假二分类：\n\n其中，K 表示 K 个判别器，表示第 k 个判别器对应的时域信号序列的长度（比如多精度 multi-resolution 判别器，不同判别器下降采样倍数不同，对应的时域信号长度也是不同的）， 表示当前第 t 个信号点在第 k 个判别器的分类预测（0 表示被分类为解码后的音频，1 表示真实的原始音频），x 表示真实的音频， 表示经过编解码器这个生成器之后恢复出的音频。因此损失函数的前半部分是为了让真实音频更倾向于被预测为 1，后半部分让合成音频更倾向于被预测为 0 。\n生成器的损失函数是为了让生成器的输出被分类为 1 类别，以达到以假乱真的目标，损失函数形式为：\n\n训练目标中还增加了 GAN 中常用的 feature matching 损失函数和多尺度频谱重建的损失函数。feature matching 就是让生成器恢复出的音频和真实的音频，在判别器的中间层上达到相近的分布，用表示在中间层上进行求和，feature matching 的损失函数为：\n\n多尺度频谱重建的损失函数形式为：\n表示尺度因子为 （STFT 的窗长为 s，跳长 hop_length 为 s/4）时第 t 帧的 64 维梅尔谱特征。分别对生成器输出的音频和真实的音频计算上述梅尔特征，在原始数值上计算 范数的损失函数，在取对数之后的数值上计算损失函数，两者间的权重系数是与尺度因子相关的。\n综上所示，GAN 的训练损失函数虽然庞杂，但是意义上是十分清晰的，将以上函数求加权和即可作为 GAN 的最终损失函数：\n\nSoundStream 实验中，。\n联合压缩与增强音频压缩（音频编码）和音频的降噪增强通常是两个不同的模块，在传统的音频处理流程中，音频增强模块通常位于音频编码前或者音频解码后，两个模块的时延是累加的。SoundStream 能够同时进行音频的编解码和降噪增强，并且不会增加系统的时延。\nSoundStream 除了可以在不同的比特率下工作外，另外的灵活之处在于推理时可以选择降噪和不降噪两种模式。在模型中增加一个条件变量 denoise，denoise 为 false 时任何音频数据都可以拿来训练，denoise 为 true 时必须同时提供音频的原始版和降噪版来训练，因此只有在条件变量 denoise 置为 true 的时候模型才具有降噪的功能。\n为了避免模型在 denoise = true 的时候对本来就干净无噪声的音频带来损伤，训练数据中还必须包含一些干净音频，在训练时 denoise = true 或 false 均可，让模型在有噪声/无噪声的条件下都具有效果的保证。\n从 SoundStream 的编解码器图例中可以看到一个 FiLM 的模块，表示特征级别的线性调制（Feature-wise Linear Modulation），在编码器中使用时位于 embedding 之前（编码前进行降噪），在解码器中使用时输入是 embedding（编码后进行降噪），论文验证了在图中位置的效果是最好的。\nFiLM 的输入是上一层网络的特征，如果用表示前一层第 n 帧激活值的第 c 个 channel 的输出值，线性调制过程可以表示为：\n\n其中, 分别表示加权系数和偏置，计算过程：前一层每帧对应一个二维 one-hot 向量，代表该时间位置是否进行降噪，然后二维 one-shot 经过线性层可以得到每个位置的降噪程度系数 (denoising level)，输出即为和。这么设计能够让模型在不同时间进行不同层级的降噪。\n\nSoundStream 评测评测准备评测数据集评测覆盖多种类型的音频，包括干净和带噪的语音和音乐，都是 24kHz 采样率。干净的语音来自 LibriTTS，带噪的语音是将 LibriTTS 和 freesound 里的噪声叠加，叠加时噪声的增益系数在 -30dB 和 0 dB 之间；音乐数据来源是 MagnaTagATune；论文还采集了真实场景的音频数据集，覆盖了近场、远场(带混响)和背景噪声的音频。相当于共四个测试集，每个测试集是 50 个待测样本。\n评测指标模型最终评测的指标采用前文所述的 MUSHRA 分数，评测人母语均为英语，戴耳机进行评测。但是在模型训练和调参时，留出一个验证集，在上面计算客观指标进行模型评价，可以用 PESQ 和 POLQA 的，本文选择的是开源的 ViSQOL 评测指标。\n评测基线Opus 是传统的音频编解码方法，支持 4kHz ~ 24 kHz 的采样率和 6 kbps ~ 510 kbps 的比特率，在 Youtube 流媒体上都在使用。另外 EVS (增强语音服务) 也是一种新编解码方法，支持 4kHz ~ 20 kHz 的采样率和 5.9 kbps ~ 128 kbps 的比特率。Google 还提出了基于自回归模型的 Lyra 编解码器，可以在 3 kbps 的低比特率下使用。本文将以上三种方法作为基线。\n实验结果不同比特率下的结果\n\n\n\n其中 scalable 的 SoundStream 代表一个支持多比特率的模型，不带 scalable 的模型表示给当前比特率专门训练的模型，可以看出模型是否 scalable 差别不大，尤其是高比特率下几乎无差别。相同比特率下，SoundStream 碾压其他三个基线模型。\n不同类型音频的结果\n\n\nSoundStream @ 3kbps 相当于 EVS @ 9.6kbps 和 Opus@12kbps，SoundStream@6kbps 相当于 Opus @ 16kbps 和 EVS @ 13.2kbps，SoundStream @ 12kbps 超过了 Opus @ 20kbps 和 EVS @ 16.4kbps。普遍性地，编解码后恢复的音频，MUSHRA 分数上：干净语音 &gt; 真实场景音频 &gt; 带噪语音 &gt; 音乐。\n消融实验神经网络编码器的重要性如果将编码器部分修改为普通的 fbank 特征（类似于一代 Lyra），再训练 RVQ 和解码器模块，此时的客观指标 ViSQOL 从 3.96 降低至 3.33；但是如果增加了神经网络结构的编码器，3 kbps 比特率下的 ViSQOL 也有 3.76，说明编码器采用神经网络结构是非常有必要的。\n模型参数量大小的影响\n\n\n\n从实验结果可以看出，建议使用相对轻量级的编码器和参数量更多的解码器。\nVQ 参数配置的影响\n\n\n假设共个量化器，每个量化器的 codebook 大小为 N，那么每帧 embedding 编码后需要比特来存储，比特率和正相关。表格中给出了相同比特率下的不同参数配置，可以看出量化器层数不必太多，每层的 codebook 大小更大时，模型的效果会更好；但同时也能看出，80 层深层的 1-bit 量化器，也能够达到较好的效果，验证了 RVQ 深层网络的有效性。\n模型的时延计算\n\n\n前文说明过，模型的时延主要取决于编码器的降采样倍数，降采样倍数越大，时延越大。表格中给出了详细的对比结果，能够看出，降采样倍数越大，时延越大，但同时模型需要的量化层数明显降低，编解码器的实时率会随之提高（因为每个 embedding 对应的真实时间更长），因此在实际场景中需要在时延和实时性之间进行 trade-off。\n联合的音频降噪和压缩该评测将联合降噪压缩与分别的降噪和压缩进行对比。降噪和压缩分开时，压缩采用 SoundStream 模型，降噪采用 SEANet 模型，关于降噪和压缩模型的使用顺序，分别使用先降噪（编码前）后压缩、先压缩后降噪（解码后）两种策略。评测数据集使用的是 24kHz 的 VCTK，没有被用于 SoundStream 和 SEANet 的训练。分别在0，5,10,15 dB 四个配置下评测：\n\n\n\n联合的压缩和降噪略差于其他两种实验配置，其他两种实验表明顺序带来的影响相差不大。SoundStream 的优势在于一个模型两种功能，简约而且省算力，并且和分开的两个模型在最终结果上相差不大。\n\n参考文献/链接\n官方博客: https://opensource.googleblog.com/2022/09/lyra-v2-a-better-faster-and-more-versatile-speech-codec.html\n示例音频: https://google-research.github.io/seanet/soundstream/examples\n官方开源: https://github.com/google/lyra\n非官方实现（PyTorch）Lucidrains: https://github.com/lucidrains/audiolm-pytorch/blob/main/audiolm_pytorch/soundstream.py\n非官方实现（Pytorch）wesbz: https://github.com/wesbz/SoundStream\nLyra v1: Kleijn, W. Bastiaan, et al. “Generative Speech Coding with Predictive Variance Regularization.” arXiv preprint arXiv:2102.09660 (2021). [pdf]\nAudioLM: Borsos, Zalán, et al. “Audiolm: a language modeling approach to audio generation.” arXiv preprint arXiv:2209.03143 (2022). [pdf]\nMusicLM: Agostinelli, Andrea, et al. “MusicLM: Generating Music From Text.” arXiv preprint arXiv:2301.11325 (2023). [pdf]\nEMA 训练 codebook 1: Van Den Oord, Aaron, and Oriol Vinyals. “Neural discrete representation learning.” Advances in neural information processing systems 30 (2017). [pdf]\nEMA 训练 codebook 2: Razavi, Ali, Aaron Van den Oord, and Oriol Vinyals. “Generating diverse high-fidelity images with vq-vae-2.” Advances in neural information processing systems 32 (2019). [pdf]\nJukebox: Dhariwal, Prafulla, et al. “Jukebox: A generative model for music.” arXiv preprint arXiv:2005.00341 (2020). [pdf]\nFiLM: Perez, Ethan, et al. “Film: Visual reasoning with a general conditioning layer.” Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 32. No. 1. 2018. [pdf]\nViSQOL 指标: Chinen, Michael, et al. “ViSQOL v3: An open source production ready objective speech and audio metric.” 2020 twelfth international conference on quality of multimedia experience (QoMEX). IEEE, 2020. [pdf]\n\n参考讲解视频\n","categories":["论文笔记"],"tags":["音频生成","音频编解码"]},{"title":"音频编解码 | Encodec","url":"/2023/01/15/meta_encodec/","content":"Encodec 是 Meta AI 于 2022 年 10 月份发表的神经网络音频编解码方法，具有比之前 Google 的 SoundStream 更优的效果。思想上和 SoundStream 几乎没有差别，沿用了 Encoder-Decoder 结构和 VQ 向量量化方法。本文对 Encodec 与 SoundStream 一致的部分不予赘述，只分析相关的改进部分。SoundStream 的论文解读详见链接。\n\n\n\n\n\n会议/期刊\n年份\n题目\n链接\n\n\n\narxiv\n2022\nHigh Fidelity Neural Audio Compression\nhttps://arxiv.org/abs/2210.13438\n\n\n\n\n神经网络编解码器对音频的压缩通常是有损的，需要解决两大问题：第一，覆盖各种各样的音频，即泛化性强，不能过拟合训练集、不能只在某种类型的音频上有好的效果；第二，高效地进行音频压缩，体现在实时性（从时间的角度）和低比特率（从数据大小的角度）两方面。\n对于 Meta AI 提出的 Encodec，为了提高模型的泛化性：一方面模型训练时使用更大更多样的训练数据集，另一方面在训练时使用了 GAN 的思想，引入判别器使得解码恢复的音频质量更高；为了提高模型的高效性：一方面对模型的复杂度限制在单核 CPU 上保持实时，另一方面引入 RVQ（残差向量量化）和熵编码进行量化，最大程度降低音频压缩后的比特率。\nEncodec 模型结构Encoder-Decoder\n\n如上图所示，Encodec 采用和 SoundStream 几乎完全一样的 Encodecor-Decoder 结构。对于 24 kHz 的音频，Encodec 和 SoundStream 一样，经过 Encoder 之后进行了 320 倍的降采样，帧率降低至 24000 / 320 = 75 Hz。Encodec 还增加了对 48 kHz 音频压缩的支持，经过 320 倍降采样后帧率降低至 150 Hz。\nEncodec 支持流式和非流式两种场景，流式适合于低时延的应用，非流式适合音频质量要求更高的应用：设每个一维卷积的卷积核大小为 K，跳步 stride 为 S，需要 padding  个时间步，非流式时将 padding 平分到第一个时间步之前和最后一个时间步之后；对于流式场景，所有的 padding 都放在第一个时间步之前。非流式场景下，Encodec 将音频按照 1s 进行片段的切分，每两个片段之间保留 10 ms 的重叠（避免毛刺噪声）；音频在输入编码器前需要 normalize，解码器的输出再反归一化（de-normalize），因此编码时还需要将 normalize 时的均值方差信息传送到解码器端。\nRVQ 残差向量量化Encodec 也用到了 RVQ 多阶段量化的方法，将多个 RVQ 模块叠加，可以在训练或推理时经过不同个数的量化器模块，实现不同的比特率，具有更高的灵活性。SoundStream 论文笔记中我们介绍了 RVQ 的 codebook 初始化、参数更新方法和 EMA（指数移动平均）的训练技巧，此处对 RVQ 的其他细节进行补充。\nStraight-Through Estimator参考 VQ-VAE 论文中的表述，设 Encoder 的输出为 ，经过向量量化器之后， 会被映射到 codebook 中和  欧氏距离最小的向量 ，得到 。用  代表 codebook 码本的大小，即包含向量的总数，那么数学形式为：\n\n\n\n\n上图是 VQ-VAE 的图例，可以作为 Encoder + VQ + Decoder 框架的典型结构，本文中 RVQ 里的每个 quantizer 都是按照 VQ-VAE 中的 VQ 方法来实现的。\n问题是：从量化前的 Encoder 输出 ，到量化后的码本中的向量  ，是一个求最值（欧氏距离最小）的操作，没有定义反向传播时梯度的计算方法；Encodec 采用了和 VQ-VAE 一样的操作，直接将  的梯度拷贝给 ，属于一种 Straight-Through Estimator 的梯度更新策略。\n\nStraight-Through Estimator 在 Yoshua Bengio 的论文 中有详细的介绍，简单理解就是将神经网络反向传播时中某一处的梯度直接作为另外一处的梯度值，相当于这两处之间的网络近似为恒等函数（Identity Function）。VQ 训练 codebook 中的向量是 Straight-Through Estimator 方法的典型应用之一，Encodec 也采用了该方法。\n\n下面给出的示例代码是 VQ 向量量化的 Pytorch 实现。\n\n [点此展开] 扩展：VQ 代码示例及讲解 \n\ndef forward(self, latents: Tensor) -&gt; Tensor:    # 1. 输入 latents 是 encoder 的输出    latents = latents.permute(0, 2, 3, 1).contiguous()    # [B x D x H x W] -&gt; [B x H x W x D]    latents_shape = latents.shape    flat_latents = latents.view(-1, self.D)  # [BHW x D]    # 2. 计算各 latent 向量和 codebook 各 embedding 之间的欧氏距离    # Compute L2 distance between latents and embedding weights    dist = torch.sum(flat_latents ** 2, dim=1, keepdim=True) + \\           torch.sum(self.embedding.weight ** 2, dim=1) - \\           2 * torch.matmul(flat_latents, self.embedding.weight.t())             # [BHW x K]    # 3. 根据欧式距离最小的原则选择相应的 codebook index    # 注意此处使用了 torch.argmin 函数，这个函数操作是不可导的    # 本代码采用的是 straight-through estimator    # Get the encoding that has the min distance    encoding_inds = torch.argmin(dist, dim=1).unsqueeze(1)  # [BHW, 1]    # 4. 根据获取的 index 得到量化后的 embedding    # Convert to one-hot encodings    device = latents.device    encoding_one_hot = torch.zeros(encoding_inds.size(0), self.K, device=device)    encoding_one_hot.scatter_(1, encoding_inds, 1)  # [BHW x K]    # Quantize the latents    quantized_latents = torch.matmul(encoding_one_hot, self.embedding.weight)  # [BHW, D]    quantized_latents = quantized_latents.view(latents_shape)  # [B x H x W x D]    # 5. 计算 VQ 相关的损失函数    # Compute the VQ Losses    # # 对应于「损失函数分析」中的第三项损失函数    commitment_loss = F.mse_loss(quantized_latents.detach(), latents)    # # 对应于「损失函数分析」中的第二项损失函数    embedding_loss = F.mse_loss(quantized_latents, latents.detach())    vq_loss = commitment_loss * self.beta + embedding_loss    # 6. 关键的一步：straight-through estimator    # Add the residue back to the latents    quantized_latents = latents + (quantized_latents - latents).detach()    return quantized_latents.permute(0, 3, 1, 2).contiguous(), vq_loss  # [B x D x H x W]\n\n计算 Encoder 输出在 codebook 中距离最近向量 index 的代码是：\nencoding_inds = torch.argmin(dist, dim=1).unsqueeze(1)\ntorch.argmin 是无法求导的；因此，体现 Straight-Through Estimator 思想的关键代码是：\nquantized_latents = latents + (quantized_latents - latents).detach()\n这行代码表示：正向传播时，量化后结果 quantized_latents 保持不变；但在反向传播时，detach()的部分梯度为 0，因此量化后的 quantized_latents 和输入的量化前的 latents 梯度相同，这就是 Straight-Through Estimator 的思想，解决了 VQ 在计算欧氏距离最小的 codebook 编号时 argmin 函数不可导的问题。事实上，VQ 还有其他方式规避不可导的问题，比如基于 Gumbel-Softmax 的量化，后续会在其他文章中具体介绍。\n\n\n\n\nVQ 的损失函数分析VQ 的损失函数一般包含三部分，如下面公式所示，后文将详细介绍。\n\n\n重建损失函数 ：用于衡量 Encoder 输入（编码前）和 Decoder 输出（解码后）之间的差异。 只影响 Encoder 和 Decoder 的参数更新，由于 Straight-Through Estimator 直接将  的梯度拷贝给 ，因此  不会影响 VQ 中 codebook 的各向量。\n\nVQ 的 codebook 损失函数：第二项  用于缩小 codebook 中的 embedding 向量和 Encoder 的输出之间的距离，但是只用来更新 codebook 中的向量，对 Encoder 的参数不产生作用，因为增加了 （stop-gradient）的限制，相当于在 Encoder 输出固定的情况下，让 codebook 中的向量和 Encoder 输出更接近。\n\nVQ 的 commitment 损失函数：codebook 损失函数是从更新 codebook 的角度缩小  与  的距离，VQ-VAE 论文中引入了第三项 ，称为 commitment loss，相当于固定 codebook 向量（增加了  的限制），只更新 Encoder 的参数，使得  更接近于与其最近的 codebook 向量。这是为了避免  选择与之距离最近的 codebook 向量时，在不同的向量中波动。设  表示第 c 层量化的输入， 表示  在第 c 层中距离最近的 codebook 向量，则 RVQ 整体的 commitment loss 可以定义为：\n\n\n\n\n使用 commitment loss 的动机Encoder 输出向量和 codebook 码本向量处于相同空间中，两者都通过梯度下降算法进行训练，但是更新速度可能不同。如果 Encoder 参数的训练速度比 codebook 快，那么 Encoder 将会不断地调整其输出向量的位置，但 codebook 向量不能及时更新，会导致 Encoder 的输出空间不断地扩大，参数更新过度“自由”，出现以下问题：1) 过拟合：Encoder 输出空间不断扩大，有更多的自由度来适应训练数据，导致过拟合训练数据；2) 训练不稳定：Encoder 的输出在选择与之距离最近的 codebook 向量时“反复横跳”，导致训练不稳定、难以收敛等问题。因此，增加 commitment loss 后能够控制编码器输出的向量，将其映射到最近的码本向量，避免过拟合和训练不稳定问题，从而提高模型的泛化能力和稳定性。\n\n三部分损失函数各自只负责 VQ 中一部分参数的更新：第一项用来优化 Encoder 和 Decoder 不用来优化 codebook，第二项只用来优化 codebook 不涉及 Encoder 和 Decoder，第三项只用来优化 Encoder 不涉及 codebook 和 Decoder。反向总结，Encoder 由第一项和第三项损失函数共同优化，codebook 由第二项损失函数来优化，Decoder 只由第一项损失函数优化。\n\n\n语言建模与熵编码Encoder-Decoder 结构和 RVQ 的使用，都是在 SoundStream 论文中已经使用的方法，那么：Encodec 相比于 SoundStream 的优越之处究竟体现在哪里呢？本部分介绍 Encodec 提出的一些新方法，可供借鉴。\n语言建模Encodec 使用一个轻量级 Transformer 语言模型，共包含 5 层 TransformerEncoder，其中 Self-Attention 的 head 和  维度分别为 8 和 200，Positionwise Feedforward 层的隐层大小为 800，不使用 dropout。每个 Self-Attention 层的感受野是因果的，只关注过去 3.5 秒的信息；同时，Encodec 还通过改变正余弦绝对位置编码（positional embedding）的偏移量（offset），来模拟当前音频片段来自于长音频中间部分的情况。\nEncoder 的降采样倍数固定时，音频压缩的比特率只与 RVQ 包含的量化层数  有关，对应于  个 codebook。\n\n设  时刻 Encoder 的输出为 ，在  个 codebook（codebook 大小均设为 ，用  比特来表征 codebook 中的向量编号）中对应的编号分别为 。\n将这  个编号分别用各自 codebook 对应的 embedding table 映射到连续空间中得到  维度的 embeddings，再将个 embeddings 相加，得到 。\n\nTransformer 语言模型是在  这个层面上进行建模的，根据历史信息 预测得到  ，送入  个全连接层，对应于  类的分类任务。Encodec 引入 Transformer 语言建模后，第  时刻每个 codebook 上的编号概率分布直接通过 Transformer 的历史信息（时刻及之前的输入）一次性预测得到，实现较快的推理。\n\n训练时，第个全连接层经过 softmax 之后的预测目标是第个 RVQ 量化后的 token 编号。\n推理时，第个全连接层经过 softmax 后的输出，是第个 codebook 上  个编号的概率分布。\n\n原本的 codebook 编号是定长编码，比如  时，每个编号使用 10 个 bit 编码。通过利用 Transformer 模型预测的编号概率分布，可以使用熵编码对量化后的 codebook 编号进行二次压缩，进一步降低比特率。\n熵编码熵编码是一种根据数据的统计特征（比如频率分布）将数据编码为变长编码的技术，核心思想是将出现频率更高的符号用更短的编码来表示，常见的熵编码方法包括霍夫曼编码和算术编码。相比于霍夫曼编码，算术编码能够更有效地压缩数据（参考链接），但同时也需要更高的计算成本。Encodec 使用的是基于区间的算术编码，也可以称为区间编码。在了解区间编码之前，可以先回顾下定长编码、霍夫曼编码、算术编码三者的原理。\n\n [点此展开] 知识点 1：定长编码 \n\n参考链接。定长编码是指每种符号使用相同 bit 长度进行编码，以一段英文字符串为例：”AABABCABAB” 共 10 个字符，A 出现了 5 次，B 出现了 4 次，C 出现了 1 次，对应的频率分别为 [0.5, 0.4, 0.1]。定长编码时，ABC 3 种符号，每种符号至少需要  个 bit 来表示，所以字符串编码后的长度为 。但是这种编码方式是存在冗余的，因为 2 bit 实际上最多能够用来表征 4 种符号。根据信息学理论的香农定理，字符串 “AABABCABAB” 的信息熵为：\n因此，每个字符平均需要  个 bit 编码，整个字符串最少只需要  个 bit 来表示。\n\n\n\n [点此展开] 知识点 2：霍夫曼编码 \n\n参考链接。如果采用霍夫曼编码，对于出现频次更少高的 A，可以使用更短的编码，如下图所示，根据频率的高低（作为权重）构建霍夫曼树：\n\n\n(1) B 和 C 的权重（频率）最小，对应的两个结点形成一个新的二叉树，根结点的权重为 B 和 C 的权重之和 0.5；(2) A 的权重也是 0.5，和 B/C 的根结点再形成一个新的二叉树；霍夫曼编码时，左子树统一编码为 0，右子树统一编码为 1；因此，A 的编码为 0，B 的编码为 10，C 的编码为 11。使用霍夫曼编码后，整个字符串的编码长度为 。\n霍夫曼编码相比于定长编码，将所需 bit 数从 20 降低至 15，但是与香农定理的熵极限值还有差距。这是因为，霍夫曼仍然采用整数个 bit 对每个符号进行编码，仍然存在冗余，比如 A 和 B 两个符号，出现概率分别为 0.4 和 0.1，却都采用了相同 bit 的编码。\n\n\n\n [点此展开] 知识点 3：算术编码 \n\n参考链接。为了帮助更好地理解 Encodec 所用的区间编码，下面先对字符串 “AABABCABAB” 进行算术编码。\n算术编码首先对 [0, 1] 区间根据概率进行划分：(1) 第一次划分：A: [0, 0.5), B:[0.5, 0.9), C:[0.9, 1]第 1 个字符为 A，那么首先选中 A 的区间 [0, 0.5) 作为新目标区间，按照概率进行划分：(2) 第二次划分：A:[0, 0.25), B: [0.25, 0.45), C:[0.45, 0.5)下一个出现的字符仍然是 A，继续按照概率对 [0, 0.25) 区间进行划分：(3) 第三次划分：A:[0, 0.125), B:[0.125, 0.225), C:[0.225, 0.25)下一个出现的字符是 B，继续按照概率对 [0.125, 0.225) 区间进行划分：(4) 第四次划分：A:[0.125, 0.175), B: [0.175, 0.215), C:[0.215, 0.225)依此类推，直到最后一个字符 B，下表给出了每个字符对应的目标区间。\n\n\n\n\n\n当前字符\n当前目标区间\n\n\n\nA\n[0, 0.5)\n\n\nA\n[0, 0.25)\n\n\nB\n[0.125, 0.225)\n\n\nA\n[0.125, 0.175)\n\n\nB\n[0.15, 0.17)\n\n\nC\n[0.168, 0.17)\n\n\nA\n[0.168, 0.169)\n\n\nB\n[0.1685, 0.1689)\n\n\nA\n[0.1685, 0.1687)\n\n\nB\n[0.1686, 0.16868)\n\n\n完成上述操作之后，最终的目标区间是 [0.1686, 0.16868)，在其中任选一个二进制表示最短的小数，比如 0.16864，二进制为：0.00101011001011，只保留小数点之后的二进制编码：00101011001011，bit 长度为 14 位，比哈夫曼编码还要少 1 位。算术编码的解码也很直接，将二进制编码还原为小数 0.16864，根据符号的概率值得到区间分布，然后基于 0.16864 所处的区间位置，对应的字符串是唯一的，反向对应得到字符串为 “AABABCABAB”。\n总结来说，算术编码的思想是：最终目标区间的范围越大，二进制编码越短；因此，高频符号需要对应更大的区间，低频符号对应更小的区间。\n\n\n\n区间编码实际上和算术编码的思想基本一致，都是基于符号的概率分布，对某个区间范围进行一步一步的划分（参考链接）。只不过算术编码选用的是 [0, 1] 区间，而区间编码通常是在整数域的某个范围内进行划分。算术编码在划分时，区间的左右边界是小数，因此区间划分受限于小数表示的精度；区间编码需要预先设定一个范围足够大的整数范围，也会受到计算机架构和不同数据类型的影响和限制。同样地，以字符串 “AABABCABAB” 为例给出区间编码的示例过程：\n\n(0) 首先预先设定一个数值范围，因为最终的编码结果也是 bit 度量的，与整数的二进制表示密切相关，所以通常使用二进制整数设定数值，比如正整数表示的最大范围 [0, ] = [0, 2147483647]\n(1) 第一次区间划分，A: [0, 1073741823), B: [1073741823, 1932735282), C: [1932735282, 2147483647]第 1 个字符为 A，那么首先选中 A 的区间 [0, 1073741823) 作为新目标区间，按照概率进行划分：\n(2) 第二次区间划分，A: [0, 536870911), B: [536870911, 966367640), C: [966367640, 1073741823)下一个出现的字符仍然是 A，选中 A 的区间 [0, 536870911) 作为新目标区间，按照概率进行划分：\n(3) 第三次区间划分，A: [0, 268435455), B: [268435455, 483183820), C: [483183820, 536870911)下一个出现的字符是 B，选中 B 的区间 [268435455, 483183820) 作为新目标区间，按照概率进行划分：\n(4) 第四次区间划分，A: [268435455, 375809637), B: [375809637, 461708983), C: [461708983, 483183820)依此类推，直到最后一个字符 B，下表给出了每个字符对应的目标区间。\n\n\n\n\n\n\n当前字符\n当前目标区间\n\n\n\nA\n[0, 1073741823)\n\n\nA\n[0, 536870911)\n\n\nB\n[268435455, 483183820)\n\n\nA\n[268435455, 375809637)\n\n\nB\n[322122546, 365072218)\n\n\nC\n[360777250, 365072218)\n\n\nA\n[360777250, 362924734)\n\n\nB\n[361850992, 362709985)\n\n\nA\n[361850992, 362280488)\n\n\nB\n[362065740, 362237538)\n\n\n完成上述操作之后，最终的目标区间为 [362065740, 362237538)，其中\n\n362065740 的二进制表示（31 bit）为：0010101100101001010111101001100\n362237538 的二进制表示（31 bit）为：0010101100101110100111001100010\n\n找到上面两个整数的公共前缀为0010101100101, 从第 14 个 bit 开始出现不同，所以可选择 0010101100101100000000000000000，对应的十进制表示为 362151936。实际上在进行压缩编码时，只考虑公共前缀部分，最后一个非零 bit 位之后的零统统可以忽略，这样可以用最少的 bit 数表示出编码结果所在的区间。\nEncodec 论文中指出，由于不同计算机体系架构表征浮点小数时存在细微差异，Transformer 语言模型预测 codebook 编号时的概率值可能会存在精度误差（ 上以上）。因此，为了去除精度误差影响，Encodec 将所有的概率值精度控制在  量级（具体部署可能需要根据应用场景修改），区间编码预设的区间范围在  以内。具体代码可参考：https://github.com/facebookresearch/encodec/blob/main/encodec/quantization/ac.py 。\n训练目标Encodec 的训练目标包含重建损失函数、GAN 的损失函数以及 RVQ 的 commitment loss。\n重建损失函数包含时域损失函数  和频域损失函数  两部分，其中时域采用  损失函数，频率采用  和  两个损失函数。 表示音频的原始波形， 表示预测时域波形； 表示不同参数的 STFT 提取得到的 64 维的梅尔特征， 和 SoundStream 中的尺度因子  作用一样。\n\n判别器损失函数为了提高解码器音频的音质，Encodec 也采用了 SoundStream 中的 GAN 训练思想，整个编解码器作为生成器，然后增加多个判别器。相比于 SoundStream 使用的 STFT 判别器，Encodec 将其扩充为多尺度的 STFT（Multi-Scale STFT）判别器，如下图所示：\n\n\n每个 STFT 内部结构基本与 SoundStream 中介绍的相同，只不过在计算 STFT 时，采用了五组不同的窗长 [2048, 1024, 512, 256, 128]。此外，Encodec 扩展了对 48 kHz 采样率的支持，窗长相应地进行了加倍，每两个 batch 数据更新一次判别器的参数；Encodec 还增加对双通道音频的支持，两个通道独立处理即可。GAN 的生成器和判别器损失函数，以及增加的 feature matching 损失函数与 SoundStream 相同，不再赘述。\n\n论文 trick：论文发现判别器更倾向于优化解码器端的参数，为了弱化这一问题，在模型参数更新时，24 kHz 情况下解码器参数更新进行 2/3 概率的加权，48 kHz 情况下解码器权重系数为 0.5。和 SoundStream 一样，通过控制训练过程中音频经过的 RVQ 层数，可以实现不同比特率的编解码：24 kHz 采取 1.5/3/6/12 kbps 四种不同的比特率，48 kHz 采取 3/6/12/24 kbps 的比特率。\n\n整体的损失函数 包含了生成器和判别器两个损失函数， 是 RVQ 的 commitment loss。\n损失函数均衡器Encodec 提出了一种损失函数均衡器（Loss Balancer），用于提高训练的稳定性。Loss Balancer 能够根据不同损失函数传回的梯度大小进行规整，避免参数更新受到单个损失函数梯度的过度影响。假设损失函数  只与模型的输入  有关，定义以下两个数值： 表示损失函数  相对于  的梯度， 表示梯度  在最后一批训练数据上的指数移动平均（EMA）。给定各种损失函数的权重 ，定义下面的新梯度：\n\n在参数更新时，反向传播的梯度使用  而不是原本的 。论文中 , EMA 的 。除了 VQ 的 commitment loss  之外（commitment loss 与编解码器的输出无关），其他的损失函数加入到损失函数均衡器中，所以上式中  的 。\n实验与结论实验准备数据集：24 kHz 在干净语音、带噪语音和音乐等各种音频上评测；48 kHz 只在音乐音频上评测。干净语音来自于 DNS 比赛和 Common Voice 数据集；音乐音频来自于 Jamendo 数据集；其他音频来自 AudioSet 和 FSD50K。\n\n\n基线：为了更好地和前人的工作进行全方位对比，基线包括 Opus、EVS、MP3 以及 Google 提出的 Lyra v2（SoundStream），其中 SoundStream 使用了官方实现版和优化版（包括判别器中间层增加了 LayerNorm，被验证能够有效提升音质）。\n评测指标：主观评测使用 MUSHRA，客观评测适用 ViSQOL 和 SI-SNR 两种。\n实验结果不同比特率下的音质对比\n\n从图中可以看出，Encodec 的效果是明显好于 SoundStream 模型的，采用熵编码能够达到稍微更好的效果。\n\n\n上图所示的表格中有更详尽的数值结果。Encodec 在 3kbps 下的音频质量甚至略好于 SoundStream 在 6bkps 下的结果；此外，经过熵编码之后，Encodec 的比特率可以进一步降低大约 25%-40%，编解码效率进一步提升。\n针对判别器的消融实验在音频和语音领域的 GAN 训练中，HiFi-GAN 使用了多周期判别器（MPD, Multi-Period Discriminator）和多尺度判别器（MSD, Multi-Scale Discriminator），SoundStream 采用的是 STFT 判别器，本文对 STFT 判别器进行了扩展，提出多尺度 STFT（MS-STFT）判别器。通过下表的实验结果，发现 MS-STFT + MPD 能够达到最好的效果，论文认为只使用 MS-STFT 即可达到很好的效果。\n\n\n\n流式编解码的对比实验表格中给出了流式/非流式编解码的效果对比：\n\n\n\n\n\n双声道音频实验对于双通道的音频，使用了算术编码的 Encodec 在 4.2 kbps 比特率下就能达到和 24 kbps 的 Opus 基本一样的效果。\n\n\n单核 CPU 下的时延与实时性与 SoundStream 相比，模型的实时率有所变差，但是 24 kHz 下仍然能够满足实际应用的实时性需求，但是 48 kHz 下实时率小于 1，只能用于音频的离线压缩场景。\n\n\n\n参考文献/链接\nEncodec 官方博客：https://ai.facebook.com/blog/ai-powered-audio-compression-technique.\nEncodec 音频示例：https://ai.honu.io/papers/encodec/samples.html\nSoundStream：Zeghidour, Neil, et al. “Soundstream: An end-to-end neural audio codec.” IEEE/ACM Transactions on Audio, Speech, and Language Processing 30 (2021): 495-507. [pdf]\nSoundStream 论文笔记：https://revospeech.github.io/2023/01/14/lyra_v2_soundstream.\n熵编码：https://zhuanlan.zhihu.com/p/390684936.\n区间编码：https://zh.wikipedia.org/wiki/区间编码.\nVQ-VAE：Van Den Oord, Aaron, and Oriol Vinyals. “Neural discrete representation learning.” Advances in neural information processing systems 30 (2017). [pdf]\nEncodec 视频讲解：https://www.youtube.com/embed/mV7bhf6b2Hs.\nMeta AI 官方代码实现：https://github.com/facebookresearch/encodec.\n\n参考讲解视频\n\n\n","categories":["论文笔记"],"tags":["音频生成","语音合成","音频编解码"]},{"title":"快速索引 | 论文笔记清单","url":"/2023/01/01/paper_list/","content":"音频编解码\n SoundStream: [链接]\n Encodec: [链接]\n\nAudioLM 系列\n AudioLM: [链接]\n MusicLM\n SingSong\n VALL-E\n VALL-EX\n\n生成模型：GAN\n GAN 的基础（一）: [链接]\n\n","categories":["论文笔记"],"tags":["音频生成","语音合成","语音识别"]},{"title":"数据集 | 开源语音数据库汇总","url":"/2023/01/07/speech-datasets-collection/","content":"This is a curated list of open speech datasets for speech-related research (mainly for Automatic Speech Recognition). Over 110 speech datasets are collected, and more than 70 datasets can be downloaded directly without further registration or application.\n\n\n\nGitHub Link: https://github.com/RevoSpeechTech/speech-datasets-collectionContributions for more speech datasets are welcome!You can issue here with new speech datasets, and the list of datasets will be updated Seasonly.\n\nNotice:\n\nThis repository does not show corresponding License of each dataset. Basically it’s OK to use these datasets for research purpose only. Please make sure the License is suitable before using for commercial purpose.\nSome small-scale speech corpora are not shown here for concision.\n\n\nData Overview\n\n\nDataset Acquisition\nSup/Unsup\nAll Languages (Hours)\nMandarin (Hours)\nEnglish (Hours)\n\n\n\ndownload directly\nsupervised\n199k +\n2110 +\n34k +\n\n\ndownload directly\nunsupervised\n530k +\n1360 +\n68k +\n\n\ndownload directly\ntotal\n729k +\n3470 +\n102k +\n\n\nneed application\nsupervised\n53k +\n16740 +\n50k +\n\n\nneed application\nunsupervised\n60k +\n12400 +\n57k +\n\n\nneed application\ntotal\n113k +\n29140 +\n107k +\n\n\ntotal\nsupervised\n252k +\n18850 +\n84k +\n\n\ntotal\nunsupervised\n590k +\n13760 +\n125k +\n\n\ntotal\ntotal\n842k +\n32610 +\n209k +\n\n\n\nMandarin here includes Mandarin-English CS corpora.\nSup means supervised speech corpus with high-quality transcription.\nUnsup means unsupervised or weakly-supervised speech corpus.\n\nList of ASR corporadirectly downloadable\n\n\nid\nName\nLanguage\nType/Domain\nPaper Link\nData Link\nSize (Hours)\n\n\n\n1\nLibrispeech\nEnglish\nReading\n[paper]\n[dataset]\n960\n\n\n2\nTED_LIUM v1\nEnglish\nTalks\n[paper]\n[dataset]\n118\n\n\n3\nTED_LIUM v2\nEnglish\nTalks\n[paper]\n[dataset]\n207\n\n\n4\nTED_LIUM v3\nEnglish\nTalks\n[paper]\n[dataset]\n452\n\n\n5\nMLS\nMultilingual\nReading\n[paper]\n[dataset]\n50k +\n\n\n6\nthchs30\nMandarin\nReading\n[paper]\n[dataset]\n35\n\n\n7\nST-CMDS\nMandarin\nCommands\n-\n[dataset]\n100\n\n\n8\naishell\nMandarin\nRecording\n[paper]\n[dataset]\n178\n\n\n9\naishell-3\nMandarin\nRecording\n[paper]\n[dataset]\n85\n\n\n10\naishell-4\nMandarin\nMeeting\n[paper]\n[dataset]\n120\n\n\n11\naishell-eval\nMandarin\nMisc\n-\n[dataset]\n80 +\n\n\n12\nPrimewords\nMandarin\nRecording\n-\n[dataset]\n100\n\n\n13\naidatatang_200zh\nMandarin\nRecord\n-\n[dataset]\n200\n\n\n14\nMagicData\nMandarin\nRecording\n-\n[dataset]\n755\n\n\n15\nMagicData-RAMC\nMandarin\nConversational\n[paper]\n[dataset]\n180\n\n\n16\nHeavy Accent Corpus\nMandarin\nConversational\n-\n[dataset]\n58 +\n\n\n17\nAliMeeting\nMandarin\nMeeting\n[paper]\n[dataset]\n120\n\n\n18\nCN-Celeb\nMandarin\nMisc\n[paper]\n[dataset]\nunsup(274)\n\n\n19\nCN-Celeb2\nMandarin\nMisc\n[paper]\n[dataset]\nunsup(1090)\n\n\n20\nThe People’s Speech\nEnglish\nMisc\n[paper]\n[dataset]\n30k +\n\n\n21\nMultilingual TEDx\nMultilingual\nTalks\n[paper]\n[dataset]\n760 +\n\n\n22\nVoxPopuli\nMultilingual\nMisc\n[paper]\n[dataset]\nsup(1.8k)unsup(400k)\n\n\n23\nLibri-Light\nEnglish\nReading\n[paper]\n[dataset]\nunsup(60k)\n\n\n24\nCommon Voice (Multilingual)\nMultilingual\nRecording\n[paper]\n[dataset]\nsup(15k)unsup(5k)\n\n\n25\nCommon Voice (English)\nEnglish\nRecording\n[paper]\n[dataset]\nsup(2200)unsup(700)\n\n\n26\nJTubeSpeech\nJapanese\nMisc\n[paper]\n[dataset]\n1300\n\n\n27\nai4bharat NPTEL2020\nEnglish(Indian)\nLectures\n-\n[dataset]\nweaksup(15.7k)\n\n\n28\nopen_stt\nRussian\nMisc\n-\n[dataset]\n20k +\n\n\n29\nASCEND\nMandarin-English CS\nConversational\n[paper]\n[dataset]\n10 +\n\n\n30\nCrowd-Sourced Speech\nMultilingual\nRecording\n[paper]\n[dataset]\n1200 +\n\n\n31\nSpoken Wikipedia\nMultilingual\nRecording\n[paper]\n[dataset]\n1000 +\n\n\n32\nMuST-C\nMultilingual\nTalks\n[paper]\n[dataset]\n6000 +\n\n\n33\nM-AILABS\nMultilingual\nReading\n-\n[dataset]\n1000\n\n\n34\nCMU Wilderness\nMultilingual\nMisc\n[paper]\n[dataset]\nunsup(14k)\n\n\n35\nGram_Vaani\nHindi\nRecording\n[paper] [code]\n[dataset]\nsup(100)unsup(1k)\n\n\n36\nVoxLingua107\nMultilingual\nMisc\n[paper]\n[dataset]\nunsup(6600 +)\n\n\n37\nKazakh Corpus\nKazakh\nRecording\n[paper] [code]\n[dataset]\n335\n\n\n38\nVoxforge\nEnglish\nRecording\n-\n[dataset]\n130\n\n\n39\nTatoeba\nEnglish\nRecording\n-\n[dataset]\n200\n\n\n40\nIndicWav2Vec\nMultilingual\nMisc\n[paper]\n[dataset]\nunsup(17k +)\n\n\n41\nVoxCeleb\nEnglish\nMisc\n[paper]\n[dataset]\nunsup(352)\n\n\n42\nVoxCeleb2\nEnglish\nMisc\n[paper]\n[dataset]\nunsup(2442)\n\n\n43\nRuLibrispeech\nRussian\nRead\n-\n[dataset]\n98\n\n\n44\nMediaSpeech\nMultilingual\nMisc\n[paper]\n[dataset]\n40\n\n\n45\nMUCS 2021 task1\nMultilingual\nMisc\n-\n[dataset]\n300\n\n\n46\nMUCS 2021 task2\nMultilingual\nMisc\n-\n[dataset]\n150\n\n\n47\nnicolingua-west-african\nMultilingual\nMisc\n[paper]\n[dataset]\n140 +\n\n\n48\nSamromur 21.05\nSamromur\nMisc\n[code]\n[dataset] [dataset][dataset]\n145\n\n\n49\nPuebla-Nahuatl\nPuebla-Nahuatl\nMisc\n[paper]\n[dataset]\n150 +\n\n\n50\nGolos\nRussian\nMisc\n[paper]\n[dataset]\n1240\n\n\n51\nParlaSpeech-HR\nCroatian\nParliament\n[paper]\n[dataset]\n1816\n\n\n52\nLyon Corpus\nFrench\nRecording\n[paper]\n[dataset]\n185\n\n\n53\nProvidence Corpus\nEnglish\nRecording\n[paper]\n[dataset]\n364\n\n\n54\nCLARIN Spoken Corpora\nCzech\nRecording\n-\n[dataset]\n1120 +\n\n\n55\nCzech Parliament Plenary\nCzech\nRecording\n-\n[dataset]\n444\n\n\n56\n(Youtube) Regional American Corpus\nEnglish (Accented)\nMisc\n[paper]\n[dataset]\n29k +\n\n\n57\nNISP Dataset\nMultilingual\nRecording\n[paper]\n[dataset]\n56 +\n\n\n58\nRegional African American\nEnglish (Accented)\nRecording\n[paper]\n[dataset]\n130 +\n\n\n59\nIndonesian Unsup\nIndonesian\nMisc\n-\n[dataset]\nunsup (3000+)\n\n\n60\nLibrivox-Spanish\nSpanish\nRecording\n-\n[dataset]\n120\n\n\n61\nAVSpeech\nEnglish\nAudio-Visual\n[paper]\n[dataset]\nunsup(4700)\n\n\n62\nCMLR\nMandarin\nAudio-Visual\n[paper]\n[dataset]\n100 +\n\n\n63\nSpeech Accent Archive\nEnglish\nAccented\n[paper]\n[dataset]\nTBC\n\n\n64\nBibleTTS\nMultilingual\nTTS\n[paper]\n[dataset]\n86\n\n\n65\nNST-Norwegian\nNorwegian\nRecording\n-\n[dataset]\n540\n\n\n66\nNST-Danish\nDanish\nRecording\n-\n[dataset]\n500 +\n\n\n67\nNST-Swedish\nSwedish\nRecording\n-\n[dataset]\n300 +\n\n\n68\nNPSC\nNorwegian\nParliament\n[paper]\n[dataset]\n140\n\n\n69\nCI-AVSR\nCantonese\nAudio-Visual\n[paper]\n[dataset]\n8 +\n\n\n70\nAalto Finnish Parliament\nFinnish\nParliament\n[paper]\n[dataset]\n3100 +\n\n\n71\nUserLibri\nEnglish\nReading\n[paper]\n[dataset]\n-\n\n\n72\nUkrainian Speech\nUkrainian\nMisc\n-\n[dataset]\n1300+\n\n\n73\nUCLA-ASR-corpus\nMultilingual\nMisc\n-\n[dataset]\nunsup(15k)sup(9k)\n\n\n74\nReazonSpeech\nJapanese\nMisc\n[paper] [code]\n[dataset]\n15k\n\n\n75\nBundestag\nGerman\nDebate\n[paper]\n[dataset]\nsup(610)unsup(1038)\n\n\nneed application\n\n\nid\nName\nLanguage\nType/Domain\nPaper Link\nData Link\nSize (Hours)\n\n\n\n1\nFisher\nEnglish\nConversational\n[paper]\n[dataset]\n2000\n\n\n2\nWenetSpeech\nMandarin\nMisc\n[paper]\n[dataset]\nsup(10k)weaksup(2.4k)unsup(10k)\n\n\n3\naishell-2\nMandarin\nRecording\n[paper]\n[dataset]\n1000\n\n\n4\naidatatang_1505zh\nMandarin\nRecording\n-\n[dataset]\n1505\n\n\n5\nSLT 2021 CSRC\nMandarin\nMisc\n[paper]\n[dataset]\n400\n\n\n6\nGigaSpeech\nEnglish\nMisc\n[paper]\n[dataset]\nsup(10k)unsup(23k)\n\n\n7\nSPGISpeech\nEnglish\nMisc\n[paper]\n[dataset]\n5000\n\n\n8\nAESRC 2020\nEnglish (accented)\nMisc\n[paper]\n[dataset]\n160\n\n\n9\nLaboroTVSpeech\nJapanese\nMisc\n[paper]\n[dataset]\n2000 +\n\n\n10\nTAL_CSASR\nMandarin-English CS\nLectures\n-\n[dataset]\n587\n\n\n11\nASRU 2019 ASR\nMandarin-English CS\nReading\n-\n[dataset]\n700 +\n\n\n12\nSEAME\nMandarin-English CS\nRecording\n[paper]\n[dataset]\n196\n\n\n13\nFearless Steps\nEnglish\nMisc\n-\n[dataset]\nunsup(19k)\n\n\n14\nFTSpeech\nDanish\nMeeting\n[paper]\n[dataset]\n1800 +\n\n\n15\nKeSpeech\nMandarin\nRecording\n[paper]\n[dataset]\n1542\n\n\n16\nKsponSpeech\nKorean\nConversational\n[paper]\n[dataset]\n969\n\n\n17\nRVTE database\nSpanish\nTV\n[paper]\n[dataset]\n800 +\n\n\n18\nDiDiSpeech\nMandarin\nRecording\n[paper]\n[dataset]\n800\n\n\n19\nBabel\nMultilingual\nTelephone\n[paper]\n[dataset]\n1000 +\n\n\n20\nNational Speech Corpus\nEnglish (Singapore)\nMisc\n[paper]\n[dataset]\n3000 +\n\n\n21\nMyST Children’s Speech\nEnglish\nRecording\n-\n[dataset]\n393\n\n\n22\nL2-ARCTIC\nL2 English\nRecording\n[paper]\n[dataset]\n20 +\n\n\n23\nJSpeech\nMultilingual\nRecording\n[paper]\n[dataset]\n1332 +\n\n\n24\nLRS2-BBC\nEnglish\nAudio-Visual\n[paper]\n[dataset]\n220 +\n\n\n25\nLRS3-TED\nEnglish\nAudio-Visual\n[paper]\n[dataset]\n470 +\n\n\n26\nLRS3-Lang\nMultilingual\nAudio-Visual\n-\n[dataset]\n1300 +\n\n\n27\nQASR\nArabic\nDialects\n[paper]\n[dataset]\n2000 +\n\n\n28\nADI (MGB-5)\nArabic\nDialects\n[paper]\n[dataset]\nunsup (3000 +)\n\n\n29\nMGB-2\nArabic\nTV\n[paper]\n[dataset]\n1200 +\n\n\n30\n3MASSIV\nMultilingual\nAudio-Visual\n[paper]\n[dataset]\nsup(310)unsup(600)\n\n\n31\nMDCC\nCantonese\nMisc\n[paper]\n[dataset]\n73 +\n\n\n32\nLahjoita Puhetta\nFinnish\nMisc\n[paper]\n[dataset]\nsup(1600)unsup(2000)\n\n\n33\nSDS-200\nSwiss German\nDialects\n[paper]\n[dataset]\n200\n\n\n34\nModality Corpus\nMultilingual\nAudio-Visual\n[paper]\n[dataset]\n30 +\n\n\n35\nHindi-Tamil-English\nMultilingual\nMisc\n-\n[dataset]\n690\n\n\n36\nEnglish-Vietnamese Corpus\nEnglish, Vietnamese\nMisc\n[paper]\n[dataset]\n500+\n\n\n37\nOLKAVS\nKorean\nAudio-Visual\n[paper] [code]\n[dataset]\n1150\n\n\nReferences\nhttps://github.com/coqui-ai/open-speech-corpora\nhttps://openslr.org/resources.php\n\n","categories":["数据集"],"tags":["语音合成","语音识别"]},{"title":"开篇 | RevoSpeech 智能语音工作指南","url":"/2023/01/01/speech-roadmap/","content":"智能语音是当今科技发展的热门方向之一。随着人工智能技术的不断进步，智能语音技术日趋成熟，在各个领域的应用也在不断增多，目前已在语音搜索、智能家居、语音助理等多个领域得进行落地，并且随着元宇宙、AIGC 等新产业的兴起焕发出新的活力。RevoSpeech 旨在推动智能语音的落地和普及，基于学术界近十年在语音处理、语音识别、语音合成等方向的技术突破，总结归纳智能语音的技术要点，密切跟进前沿科研动向的同时，展望智能语音乃至人工智能技术的未来发展。\n本文主要梳理未来一年（甚至更长时间范围）内，RevoSpeech 计划在智能语音方向的发展指南（Road Map），主要着眼于技术总结和前沿论文跟进，同时也将对智能语音领域的数据工程及应用落地等问题进行探讨。\n\n注意：本工作指南将主要着眼于语音识别和语音合成两大技术。\n\n语音识别语音数据库构建开源语音数据梳理\n整理目前语音社区内的大规模语音数据，持续跟进开源数据\n覆盖英文、中文、韩语、日语、俄语、法语等多语种语音数据\n持续更新中：speech-datasets-collection\n\n自建语音数据库数据库构建流程\n参考论文\n\nLibriSpeech: pdf, MLS (LibriVox): pdf\nGigaSpeech: pdf, WenetSpeech: pdf\nJTubeSpeech: pdf, SPGISpeech: pdf, The People’s Speech: pdf\n\n\n数据获取来源\n\nVideos: YouTube, bilibili\nPodcasts: google, apple\nMisc: archive.org\n\n\n数据获取工具\n\nytb-dlp, you-get\n\n\n数据清洗流程（待梳理）\n\n文本归一化: NeMo, Wenet, SpeechIO\n强制对齐: Kaldi, CTC, DSAlign\n音频切分与校验：Kaldi cleanup\n\n\n\n数据库构建计划v1.0 阶段\n\n目标语言：中文/英文\n数据来源：youtube / bilibili / podcast 自带字幕文件的音视频数据\n对齐模型：kaldi 使用 multi-cn + wenetspeech 的 nnet3 模型\n处理思路：基于字幕或文本进行长音频切分并进行 cleanup 处理\n方案评测：抽查数据准确度 (要求 97% 以上)，确认方案可行性\n最终产出：中文/英文各 10k 小时以上\n\nv2.0 阶段\n\n目标语言：中文/英文\n数据来源：youtube / bilibili 不带外挂字幕但存在硬字幕的视频\n额外功能：需要额外进行字幕定位和 OCR 文本识别功能\n处理思路：以 OCR 识别出的结果为伪标签，再进行 cleanup 处理\n方案评测：抽查数据准确度 (要求 97% 以上)，确认方案可行性\n最终产出：中文/英文各 10k 小时以上\n\nv3.0 阶段 (长期计划)\n\n多次迭代模型，提高 Kaldi 中文/英文对齐模型的多领域泛化能力\n扩充目标一：中文/英文的不同方言/口音的语音数据\n扩充目标二：法语/德语/西班牙/韩语/日语 等多语种的语音数据\n扩充目标三：Audio-Visual 语音和图像多模态的数据库\n\n\n传统语音识别以 Kaldi 为代表的传统语音识别，目前在学术界/产业界仍然保持着相对的优势。虽然识别准确率与端到端模型已经存在比较明显的差距，但其中声学特征、MMI 损失函数、WFST 解码器、语言模型重打分等构件在端到端语音识别中仍然是重要的技术。因此，在传统语音识别部分，RevoSpeech 将着力推进 Kaldi-Revo 项目，优化传统语音识别的训练框架，充分发挥其模型轻量、落地成本低等优势。\n训练流程优化声学特征\n理论梳理：MFCC/FBank/LPC 等声学特征的提取流程\n代码研读：Kaldi 特征提取部分的代码整理\n工程实践：C++ 动态库及接口封装知识\n\n声学模型\n核心训练流程：GMM-HMM → DNN-HMM(nnet3) → LF-MMI(chain) \n发音词典构建：在 GMM 早期加入更多可能的发音，统计训练数据的发音概率进行筛选\n前沿声学模型：\n候选结构：TDNN-F-SAN 或 Multi-Stream TDNN-F-SAN\n流式识别：保证实时性、准确性、时延低\n\n\n\n语言模型\nN-gram 语言模型：\n通用向语言模型的训练\n\n\n语言模型重打分：\n候选结构：Transformer/Transformer-XL 两种模型的双向重打分\n加速思想：词级别语言模型 → 字级别语言模型\n工程实践：Transformer-XL 参考 NVIDIA 的 Benchmark\n\n\n\n解码器\n理论学习：\n参考教程：语音识别：原理与应用(第二版), 代码研读\n\n\n热词方案：基于前缀树的方案在 Kaldi 解码器代码上实现\n解码器复现\nHMM 的 WFST Decoder 的实现方法\nCTC 的 WFST Decoder 的实现方法\n\n\n解码器优化\nBigLM Decoder 的实现方法\nAsync Decoder\nLET-Decoder\n\n\n\n开源数据集实验v1.0 阶段\n\n中文：aishell / aidatatang_200zh / MagicData-RAMC / multi_cn / WenetSpeech \n英文：LibriSpeech / TEDLium / GigaSpeech \n中英 Code-Switching：ASCEND / TALCS \n英文：尝试 grapheme 级别的 chenone 建模方式 pdf\n产出：整理对比实验结果，梳理得到 Benchmark\n\nv2.0 阶段\n\n自监督特征：Wav2vec2 / WavLM / HuBert / Data2vec \n多通道/远场：aishell4 / alimeeting / CHiME(5|6|7) \n多语种 ASR\n\nv3.0 阶段(长期计划)\n\n多模态 (AVSR)：LRS2-BBC / LRS3-TED / CMLR\n多模态自监督：AV-HuBert\n歌词识别/转写：DAMP / DALI\n多模态歌词识别：N20EM\n其他参考工作：pkwrap, apam, wav2vec-lfmmi\n\n工具化实践\nCPU 服务：vosk-server / vosk-api 实践及优化\nGPU 服务：Nvidia Triton GPU 服务化实践\nAndroid 端侧：vosk-android 实践及优化\n参考书籍：语音识别服务实战\n\n\n端到端语音识别基础知识准备\n训练准则：CTC / RNN-T / Pruned RNN-T \n模型结构：Transformer / Conformer (Squeezeformer) / Emformer / Zipformer \n多种解码方案：\nCTC：greedy search / beam search / prefix beam search\nRNN-T：greedy search / beam search / TSD / ALSD / NSC / MAES\n\n\n流式语音识别：Emformer / Zipformer \n语言模型适应：N-gram / Transformer LM 并入解码过程\n非自回归的端到端 ASR\nMask-CTC 系列模型\nParaformer\n\n\n进阶课题\n低延迟解码：RNN-T 的 FastEmit / TrimTail \n内部语言模型估计与适应：ILME / ILMA / ILMT\n热词 (context-biasing)：WFST / 前缀树\n\n\n\nASR Benchmark\n开源数据集实验\n中文：aishell / aidatatang_200zh / MagicData-RAMC / multi_cn / WenetSpeech \n英文：LibriSpeech / TEDLium / GigaSpeech \n中英 Code-Switching：ASCEND / TALCS \n自监督特征：Wav2vec2 / WavLM / HuBert / Data2vec \n多通道/远场：aishell4 / alimeeting / CHiME(5|6|7) \n多模态 (AVSR)：LRS2-BBC / LRS3-TED / CMLR\n多模态自监督：AV-HuBert\n歌词识别/转写：DAMP / DALI\n多模态歌词识别：N20EM\n\n\n\n工程化实践\nWenet Runtime\nIcefall (sherpa / sherpa-ncnn)\n预计产出：开源大规模预训练模型，提供中文/英文/中英混等语音识别服务\n\n\n语音合成深度学习生成模型\n理论基础：自回归模型 / VAE / GAN / Flow / Diffusion\n参考资料：udlbook / pml-book2\n\n数据采集与处理\n与语音识别中的数据处理基本一致，不再赘述\n强制对齐通常采用 MFA 工具进行离线对齐\n\n前端文本分析\n文本预处理：文本归一化\n中文 / 英文 g2p 模块，包含多音字词的发音预测\n基于文本的分词和停顿预测模块\n功能产出：输出带有停顿信息的音素序列\n\n声学模型\n自回归：Tacotron / Tacotron2\n非自回归：FastSpeech / FastPitch / FastSpeech2 / SpeedySpeech\n经典文章：DeepVoice 1/2/3 、Parallel tacotron 1/2\n基于 VAE：VARA-TTS / VAENAR-TTS / PVAE / GMVAE-Tacotron / VAE-TTS / BVAE-TTS\n基于 GAN：GAN exposure / Multi-SpectroGAN\n基于 Flow：Flow-TTS / Glow-TTS / RAD-TTS / Flowtron\n基于 Diffusion：Guided-TTS / Grad-TTS / Diff-TTS / PriorGrad-AM / DiffWave / FastDiff / ProDiff\n其他：DurIAN / VoiceLoop / ParaNet\n\nVocoder 声码器\n经典之作：WORLD / WaveNet / WaveRNN\nWaveRNN 系列：subscale WaveRNN / MultiBand WaveRNN / Universal WaveRNN / LPCNet\nLPCNet 进阶版：FeatherWave / Full-band LPCNet / Bunched LPCNet / Gaussian LPCNet\n基于 VAE：Wave-VAE\n基于 GAN：WaveGAN / GAN-TTS / Parallel WaveGAN / VocGAN / MelGAN / Hifi-GAN / Fre-GAN\n基于 MelGAN：MultiBand MelGAN\n通用声码器：Avocodo / BigVGAN\n\n\n基于 Flow：Parallel WaveNet / WaveGlow / WaveFlow / SqueezeWave / FloWaveNet\n基于 Diffusion：WaveGrad /  PriorGrad-Vocoder / DiffWave\n\n完全端到端语音合成\nFastSpeech 2s / EfficientTTS-Wav / Wave-Tacotron\nEATS / JETS：FastSpeech 2 + Hifi-GAN\nVITS / NaturalSpeech\n\nTTS 进阶方向\n表现力 TTS / 情感化 TTS / 个性化 TTS\n语音转换：Voice Conversion\n歌声合成：WeSinger / WeSinger2 / Diffsinger / Multi-Singer / Learn2sing 1+2\n语音编辑：RetrieverTTS / A3T\n\n工程实践\n参考项目：espnet / PaddleSpeech\n\n\n预期产出语音数据库构建\n搜集开源语音数据：speech-datasets-collection\n\nhttps://github.com/RevoSpeechTech/speech-datasets-collection\n目前持续更新中，网页版\n\n\n数据采集功能模块：speech-miner\n\n支持语音数据的自动下载 + 预处理 + 格式规范化\n\n\n数据对齐清洗模块：speech-cleaner\n\n支持语音数据的清洗处理、对齐及筛选\n\n\n字幕 OCR 提取功能模块：subtitle-extractor\n\n支持一般视频的字幕定位 + OCR 文本识别\n\n\n\n传统语音识别\nKaldi Benchmark 框架：kaldi-revo\n\n使用更准确的 nnet3 模型获取对齐结果\n支持 TDNN-F-SAN + Multi-Stream TDNN-F-SAN 模型\n支持 Transformer / Transformer-XL 重打分\n多个 egs 及 Benchmark 实验结果\n支持基于前缀树的热词方案配置\n支持 Audio-Visual 语音识别\n\n\n英文 g2p 发音词典构建：english-lexicon-builder\n\n发音词典引入 g2p 工具的多样性\n发布基于 GigaSpeech 的开源英文发音词典\n\n\nAudio-Visual 语音识别辅助模块：visual-feature-extractor\n\n多个候选模型提取图像模态的特征\n\n\n统一的语言模型重打分框架：neural-lm-rescorer\n\n支持：RNN/LSTM / SRU / Transformer / Transformer-XL / GPT-2\n支持双向重打分操作\n支持单机多卡下的大规模文本数据训练\n开源大规模语料预训练的语言模型\n加速和 Benchmark 结果参考 Nvidia 相关工作\n\n\n\n端到端语音识别\n在现有框架基础上进行功能增强\nIcefall (主): Conformer, Emformer, Zipformer \nWenet (次): U2, U2++ \nEspnet: 借鉴前沿新工作\nFairseq: 借鉴前沿新工作\n\n\n\n语音合成\n在现有框架基础上进行功能增强\nEspnet: 借鉴前沿新工作\nParallelWaveGAN: 声码器\n\n\n\n开源预训练模型\n语音数据处理\n强制对齐模型\n\n\n语音识别模型\nKaldi 声学模型\nTransformer 等强语言模型\n端到端 ASR 框架的预训练模型\n\n\n语音合成模型\n多说话人 TTS 模型\n通用声码器\n\n\n\n","categories":["技术思考"],"tags":["语音合成","语音识别"]}]