<!DOCTYPE html><html lang="zh-CN"><head><meta name="google-site-verification" content="YCdLZ428Qxbc1A-LthSxDRD0i9qhYVAMj6s4vMllnOs"><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="keywords" content="Hexo Theme Keep"><meta name="description" content="Hexo Theme Keep"><meta name="author" content="RevoSpeech"><title>数据集 | 开源语音数据库汇总 | RevoSpeech</title><link rel="stylesheet" href="/css/style.css"><link rel="shortcut icon" href="https://cdn.staticaly.com/gh/revospeech/image-hosting@master/20230228/favicon_white.jpg"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/font/css/fontawesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/font/css/regular.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/font/css/solid.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/font/css/brands.min.css"><script id="hexo-configurations">let KEEP=window.KEEP||{};KEEP.hexo_config={hostname:"revospeech.github.io",root:"/",language:"zh-CN",path:"search.json"},KEEP.theme_config={toc:{enable:!0,number:!0,expand_all:!0,init_open:!0},style:{primary_color:"#0066cc",logo:"https://cdn.staticaly.com/gh/revospeech/image-hosting@master/20230211/R-scale.jpg",favicon:"https://cdn.staticaly.com/gh/revospeech/image-hosting@master/20230228/favicon_white.jpg",avatar:"https://cdn.staticaly.com/gh/revospeech/image-hosting@master/20230211/avatar.mn776y46kc0.jpg",font_size:null,font_family:null,hover:{shadow:!0,scale:!0},first_screen:{enable:!0,header_transparent:!1,background_img:"https://cdn.staticaly.com/gh/revospeech/image-hosting@master/20230212/background-mask-grey.jpg",description:"Revive and Evolve || Towards the Future",font_color:"#E9E9E9",hitokoto:!1},scroll:{progress_bar:!0,percent:!0}},local_search:{enable:!0,preload:!0},code_copy:{},code_block:{tools:{enable:!0,style:"default"},highlight_theme:"obsidian"},side_tools:{},pjax:{enable:!0},lazyload:{enable:!0},comment:{enable:!1,use:"waline",valine:{appid:null,appkey:null,server_urls:null,placeholder:null},gitalk:{github_id:null,github_admins:null,repository:null,client_id:null,client_secret:null,proxy:null},twikoo:{env_id:null,region:null,version:"1.6.8"},waline:{server_url:"https://ixvtasha.api.lncldglobal.com",reaction:!1,version:2}},post:{author_label:{enable:!0,auto:!0,custom_label_list:["Trainee","Engineer","Architect"]},word_count:{enable:!0,wordcount:!0,min2read:!0},img_align:"center",copyright_info:!0},version:"3.6.1"},KEEP.language_ago={second:"%s 秒前",minute:"%s 分钟前",hour:"%s 小时前",day:"%s 天前",week:"%s 周前",month:"%s 个月前",year:"%s 年前"},KEEP.language_code_block={copy:"复制代码",copied:"已复制",fold:"折叠代码块",folded:"已折叠"},KEEP.language_copy_copyright={copy:"复制版权信息",copied:"已复制",title:"原文标题",author:"原文作者",link:"原文链接"}</script><meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="alternate" href="/atom.xml" title="RevoSpeech" type="application/atom+xml">
</head><body><div class="progress-bar-container"><span class="scroll-progress-bar"></span> <span class="pjax-progress-bar"></span> <i class="pjax-progress-icon fas fa-circle-notch fa-spin"></i></div><main class="page-container"><div class="page-main-content"><div class="page-main-content-top"><header class="header-wrapper"><div class="header-content"><div class="left"><a class="logo-image" href="/"><img src="https://cdn.staticaly.com/gh/revospeech/image-hosting@master/20230211/R-scale.jpg"> </a><a class="logo-title" href="/">RevoSpeech</a></div><div class="right"><div class="pc"><ul class="menu-list"><li class="menu-item"><a href="/">首页</a></li><li class="menu-item"><a href="/archives">归档</a></li><li class="menu-item"><a href="/tags">标签</a></li><li class="menu-item"><a href="/about">关于</a></li><li class="menu-item search search-popup-trigger"><i class="fas fa-search"></i></li></ul></div><div class="mobile"><div class="icon-item search search-popup-trigger"><i class="fas fa-search"></i></div><div class="icon-item menu-bar"><div class="menu-bar-middle"></div></div></div></div></div><div class="header-drawer"><ul class="drawer-menu-list"><li class="drawer-menu-item flex-center"><a href="/">首页</a></li><li class="drawer-menu-item flex-center"><a href="/archives">归档</a></li><li class="drawer-menu-item flex-center"><a href="/tags">标签</a></li><li class="drawer-menu-item flex-center"><a href="/about">关于</a></li></ul></div><div class="window-mask"></div></header></div><div class="page-main-content-middle"><div class="main-content"><div class="fade-in-down-animation"><div class="post-page-container"><div class="article-content-container"><div class="article-title"><span class="title-hover-animation">数据集 | 开源语音数据库汇总</span></div><div class="article-header"><div class="avatar"><img src="https://cdn.staticaly.com/gh/revospeech/image-hosting@master/20230211/avatar.mn776y46kc0.jpg"></div><div class="info"><div class="author"><span class="name">RevoSpeech</span> <span class="author-label">Lv1</span></div><div class="meta-info"><div class="article-meta-info"><span class="article-date article-meta-item"><i class="fa-regular fa-calendar-plus"></i>&nbsp; <span class="pc">2023-01-07</span> <span class="mobile">2023-01-07</span> </span><span class="article-update-date article-meta-item"><i class="fas fa-file-pen"></i>&nbsp; <span class="pc">2023-05-28 09:28:24</span> </span><span class="article-categories article-meta-item"><i class="fas fa-folder"></i>&nbsp;<ul><li><a href="/categories/%E6%95%B0%E6%8D%AE%E9%9B%86/">数据集</a>&nbsp;</li></ul></span><span class="article-tags article-meta-item"><i class="fas fa-tags"></i>&nbsp;<ul><li><a href="/tags/%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90/">语音合成</a>&nbsp;</li><li>| <a href="/tags/%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB/">语音识别</a>&nbsp;</li></ul></span><span class="article-wordcount article-meta-item"><i class="fas fa-file-word"></i>&nbsp;<span>1.2k 字</span> </span><span class="article-min2read article-meta-item"><i class="fas fa-clock"></i>&nbsp;<span>6 分钟</span> </span><span class="article-pv article-meta-item"><i class="fas fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span></span></div></div></div></div><div class="article-content keep-markdown-body"><p>This is a curated list of open speech datasets for speech-related research (mainly for Automatic Speech Recognition). Over <strong>110</strong> speech datasets are collected, and more than <strong>70</strong> datasets can be downloaded directly without further registration or application.</p><span id="more"></span><blockquote><p>GitHub Link: <a class="link" target="_blank" rel="noopener" href="https://github.com/RevoSpeechTech/speech-datasets-collection">https://github.com/RevoSpeechTech/speech-datasets-collection<i class="fas fa-external-link-alt"></i></a><br>Contributions for more speech datasets are welcome!<br>You can issue <a class="link" target="_blank" rel="noopener" href="https://github.com/RevoSpeechTech/speech-datasets-collection/issues">here<i class="fas fa-external-link-alt"></i></a> with new speech datasets, and the list of datasets will be updated <strong>Seasonly</strong>.</p></blockquote><p><strong>Notice:</strong></p><ol><li>This repository does not show corresponding License of each dataset. Basically it’s OK to use these datasets for research purpose only. Please make sure the License is suitable before using for commercial purpose.</li><li>Some small-scale speech corpora are not shown here for concision.</li></ol><hr><h2 id="Data-Overview"><a href="#Data-Overview" class="headerlink" title="Data Overview"></a>Data Overview</h2><table><thead><tr><th align="center"><strong>Dataset Acquisition</strong></th><th align="center"><strong>Sup/Unsup</strong></th><th align="center"><strong>All Languages (Hours)</strong></th><th align="center"><strong>Mandarin (Hours)</strong></th><th align="center"><strong>English (Hours)</strong></th></tr></thead><tbody><tr><td align="center">download directly</td><td align="center">supervised</td><td align="center">199k +</td><td align="center">2110 +</td><td align="center">34k +</td></tr><tr><td align="center">download directly</td><td align="center">unsupervised</td><td align="center">530k +</td><td align="center">1360 +</td><td align="center">68k +</td></tr><tr><td align="center">download directly</td><td align="center">total</td><td align="center">729k +</td><td align="center">3470 +</td><td align="center">102k +</td></tr><tr><td align="center">need application</td><td align="center">supervised</td><td align="center">53k +</td><td align="center">16740 +</td><td align="center">50k +</td></tr><tr><td align="center">need application</td><td align="center">unsupervised</td><td align="center">60k +</td><td align="center">12400 +</td><td align="center">57k +</td></tr><tr><td align="center">need application</td><td align="center">total</td><td align="center">113k +</td><td align="center">29140 +</td><td align="center">107k +</td></tr><tr><td align="center">total</td><td align="center">supervised</td><td align="center">252k +</td><td align="center">18850 +</td><td align="center">84k +</td></tr><tr><td align="center">total</td><td align="center">unsupervised</td><td align="center">590k +</td><td align="center">13760 +</td><td align="center">125k +</td></tr><tr><td align="center">total</td><td align="center">total</td><td align="center">842k +</td><td align="center">32610 +</td><td align="center">209k +</td></tr></tbody></table><ul><li><strong>Mandarin</strong> here includes Mandarin-English CS corpora.</li><li><strong>Sup</strong> means supervised speech corpus with high-quality transcription.</li><li><strong>Unsup</strong> means unsupervised or weakly-supervised speech corpus.</li></ul><h2 id="List-of-ASR-corpora"><a href="#List-of-ASR-corpora" class="headerlink" title="List of ASR corpora"></a>List of ASR corpora</h2><h3 id="directly-downloadable"><a href="#directly-downloadable" class="headerlink" title="directly downloadable"></a>directly downloadable</h3><table><thead><tr><th align="center"><strong>id</strong></th><th align="center"><strong>Name</strong></th><th align="center"><strong>Language</strong></th><th align="center"><strong>Type/Domain</strong></th><th align="center"><strong>Paper Link</strong></th><th align="center"><strong>Data Link</strong></th><th align="center"><strong>Size (Hours)</strong></th></tr></thead><tbody><tr><td align="center">1</td><td align="center">Librispeech</td><td align="center">English</td><td align="center">Reading</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.danielpovey.com/files/2015_icassp_librispeech.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.openslr.org/12/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">960</td></tr><tr><td align="center">2</td><td align="center">TED_LIUM v1</td><td align="center">English</td><td align="center">Talks</td><td align="center"><a class="link" target="_blank" rel="noopener" href="http://www.lrec-conf.org/proceedings/lrec2012/pdf/698_Paper.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.openslr.org/19/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">118</td></tr><tr><td align="center">3</td><td align="center">TED_LIUM v2</td><td align="center">English</td><td align="center">Talks</td><td align="center"><a class="link" target="_blank" rel="noopener" href="http://www.lrec-conf.org/proceedings/lrec2014/pdf/1104_Paper.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.openslr.org/19">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">207</td></tr><tr><td align="center">4</td><td align="center">TED_LIUM v3</td><td align="center">English</td><td align="center">Talks</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1805.04699.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.openslr.org/51">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">452</td></tr><tr><td align="center">5</td><td align="center">MLS</td><td align="center">Multilingual</td><td align="center">Reading</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2012.03411.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.openslr.org/94">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">50k +</td></tr><tr><td align="center">6</td><td align="center">thchs30</td><td align="center">Mandarin</td><td align="center">Reading</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1512.01882.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.openslr.org/18/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">35</td></tr><tr><td align="center">7</td><td align="center">ST-CMDS</td><td align="center">Mandarin</td><td align="center">Commands</td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.openslr.org/38/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">100</td></tr><tr><td align="center">8</td><td align="center">aishell</td><td align="center">Mandarin</td><td align="center">Recording</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1709.05522.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.openslr.org/33/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">178</td></tr><tr><td align="center">9</td><td align="center">aishell-3</td><td align="center">Mandarin</td><td align="center">Recording</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.11567.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="http://www.openslr.org/93/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">85</td></tr><tr><td align="center">10</td><td align="center">aishell-4</td><td align="center">Mandarin</td><td align="center">Meeting</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.03603.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="http://www.openslr.org/111/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">120</td></tr><tr><td align="center">11</td><td align="center">aishell-eval</td><td align="center">Mandarin</td><td align="center">Misc</td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://aishelltech.com/aishell_2018_eval">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">80 +</td></tr><tr><td align="center">12</td><td align="center">Primewords</td><td align="center">Mandarin</td><td align="center">Recording</td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.openslr.org/47/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">100</td></tr><tr><td align="center">13</td><td align="center">aidatatang_200zh</td><td align="center">Mandarin</td><td align="center">Record</td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.openslr.org/62/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">200</td></tr><tr><td align="center">14</td><td align="center">MagicData</td><td align="center">Mandarin</td><td align="center">Recording</td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.openslr.org/68/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">755</td></tr><tr><td align="center">15</td><td align="center">MagicData-RAMC</td><td align="center">Mandarin</td><td align="center">Conversational</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.16844.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://openslr.org/123/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">180</td></tr><tr><td align="center">16</td><td align="center">Heavy Accent Corpus</td><td align="center">Mandarin</td><td align="center">Conversational</td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://magichub.com/datasets/mandarin-heavy-accent-conversational-speech-corpus/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">58 +</td></tr><tr><td align="center">17</td><td align="center">AliMeeting</td><td align="center">Mandarin</td><td align="center">Meeting</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2202.03647.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.openslr.org/119/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">120</td></tr><tr><td align="center">18</td><td align="center">CN-Celeb</td><td align="center">Mandarin</td><td align="center">Misc</td><td align="center"><a class="link" target="_blank" rel="noopener" href="http://cnceleb.org/static/CN-Celeb_A_Challenging_Chinese_Speaker_Recognition_Dataset.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.openslr.org/82/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">unsup(274)</td></tr><tr><td align="center">19</td><td align="center">CN-Celeb2</td><td align="center">Mandarin</td><td align="center">Misc</td><td align="center"><a class="link" target="_blank" rel="noopener" href="http://aishell-cnsrc.oss-cn-hangzhou.aliyuncs.com/CN-Celeb_Multi-Genre_Speaker_Recognition.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.openslr.org/82/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">unsup(1090)</td></tr><tr><td align="center">20</td><td align="center">The People’s Speech</td><td align="center">English</td><td align="center">Misc</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2111.09344.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://mlcommons.org/en/peoples-speech/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">30k +</td></tr><tr><td align="center">21</td><td align="center">Multilingual TEDx</td><td align="center">Multilingual</td><td align="center">Talks</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2102.01757.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="http://www.openslr.org/100">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">760 +</td></tr><tr><td align="center">22</td><td align="center">VoxPopuli</td><td align="center">Multilingual</td><td align="center">Misc</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2101.00390.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/facebookresearch/voxpopuli">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">sup(1.8k)<br>unsup(400k)</td></tr><tr><td align="center">23</td><td align="center">Libri-Light</td><td align="center">English</td><td align="center">Reading</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.07875.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/facebookresearch/libri-light/tree/main/data_preparation">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">unsup(60k)</td></tr><tr><td align="center">24</td><td align="center">Common Voice (Multilingual)</td><td align="center">Multilingual</td><td align="center">Recording</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.06670.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://commonvoice.mozilla.org/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">sup(15k)<br>unsup(5k)</td></tr><tr><td align="center">25</td><td align="center">Common Voice (English)</td><td align="center">English</td><td align="center">Recording</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1912.06670.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://commonvoice.mozilla.org/en/datasets">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">sup(2200)<br>unsup(700)</td></tr><tr><td align="center">26</td><td align="center">JTubeSpeech</td><td align="center">Japanese</td><td align="center">Misc</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.09323.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/sarulab-speech/jtubespeech">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">1300</td></tr><tr><td align="center">27</td><td align="center">ai4bharat NPTEL2020</td><td align="center">English(Indian)</td><td align="center">Lectures</td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/AI4Bharat/NPTEL2020-Indian-English-Speech-Dataset">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">weaksup(15.7k)</td></tr><tr><td align="center">28</td><td align="center">open_stt</td><td align="center">Russian</td><td align="center">Misc</td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/snakers4/open_stt">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">20k +</td></tr><tr><td align="center">29</td><td align="center">ASCEND</td><td align="center">Mandarin-English CS</td><td align="center">Conversational</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2112.06223.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://huggingface.co/datasets/CAiRE/ASCEND">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">10 +</td></tr><tr><td align="center">30</td><td align="center">Crowd-Sourced Speech</td><td align="center">Multilingual</td><td align="center">Recording</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.isca-speech.org/archive/pdfs/sltu_2018/kjartansson18_sltu.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/coqui-ai/open-speech-corpora/blob/150d316869c7ba468efd1f7b473555b0c76cc5e6/README.md?plain=1#L80">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">1200 +</td></tr><tr><td align="center">31</td><td align="center">Spoken Wikipedia</td><td align="center">Multilingual</td><td align="center">Recording</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arne.chark.eu/static/spoken-wp-corpus-collection.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://corpora.uni-hamburg.de/hzsk/de/islandora/object/spoken-corpus:swc-2.0#additional-files">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">1000 +</td></tr><tr><td align="center">32</td><td align="center">MuST-C</td><td align="center">Multilingual</td><td align="center">Talks</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://aclanthology.org/N19-1202.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://ict.fbk.eu/must-c-release-v1-2/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">6000 +</td></tr><tr><td align="center">33</td><td align="center">M-AILABS</td><td align="center">Multilingual</td><td align="center">Reading</td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.caito.de/2019/01/03/the-m-ailabs-speech-dataset/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">1000</td></tr><tr><td align="center">34</td><td align="center">CMU Wilderness</td><td align="center">Multilingual</td><td align="center">Misc</td><td align="center"><a class="link" target="_blank" rel="noopener" href="http://www.cs.cmu.edu/~awb/papers/2019_Black_ICASSP.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/festvox/datasets-CMU_Wilderness">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">unsup(14k)</td></tr><tr><td align="center">35</td><td align="center">Gram_Vaani</td><td align="center">Hindi</td><td align="center">Recording</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.16973.pdf">[paper]<i class="fas fa-external-link-alt"></i></a> <a class="link" target="_blank" rel="noopener" href="https://github.com/anish9208/gramvaani_hindi_asr">[code]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.openslr.org/118/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">sup(100)<br>unsup(1k)</td></tr><tr><td align="center">36</td><td align="center">VoxLingua107</td><td align="center">Multilingual</td><td align="center">Misc</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2011.12998.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="http://bark.phon.ioc.ee/voxlingua107/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">unsup(6600 +)</td></tr><tr><td align="center">37</td><td align="center">Kazakh Corpus</td><td align="center">Kazakh</td><td align="center">Recording</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2009.10334.pdf">[paper]<i class="fas fa-external-link-alt"></i></a> <a class="link" target="_blank" rel="noopener" href="https://github.com/IS2AI/ISSAI_SAIDA_Kazakh_ASR">[code]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.openslr.org/102/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">335</td></tr><tr><td align="center">38</td><td align="center">Voxforge</td><td align="center">English</td><td align="center">Recording</td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="http://www.voxforge.org/home/downloads">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">130</td></tr><tr><td align="center">39</td><td align="center">Tatoeba</td><td align="center">English</td><td align="center">Recording</td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://downloads.tatoeba.org/audio/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">200</td></tr><tr><td align="center">40</td><td align="center">IndicWav2Vec</td><td align="center">Multilingual</td><td align="center">Misc</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2111.03945.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/AI4Bharat/IndicWav2Vec/tree/main/data_prep_scripts/pret_scripts">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">unsup(17k +)</td></tr><tr><td align="center">41</td><td align="center">VoxCeleb</td><td align="center">English</td><td align="center">Misc</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.robots.ox.ac.uk/~vgg/publications/2017/Nagrani17/nagrani17.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://mm.kaist.ac.kr/datasets/voxceleb">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">unsup(352)</td></tr><tr><td align="center">42</td><td align="center">VoxCeleb2</td><td align="center">English</td><td align="center">Misc</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.robots.ox.ac.uk/~vgg/publications/2018/Chung18a/chung18a.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://mm.kaist.ac.kr/datasets/voxceleb">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">unsup(2442)</td></tr><tr><td align="center">43</td><td align="center">RuLibrispeech</td><td align="center">Russian</td><td align="center">Read</td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.openslr.org/96/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">98</td></tr><tr><td align="center">44</td><td align="center">MediaSpeech</td><td align="center">Multilingual</td><td align="center">Misc</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.16193.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.openslr.org/108/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">40</td></tr><tr><td align="center">45</td><td align="center">MUCS 2021 task1</td><td align="center">Multilingual</td><td align="center">Misc</td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.openslr.org/103/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">300</td></tr><tr><td align="center">46</td><td align="center">MUCS 2021 task2</td><td align="center">Multilingual</td><td align="center">Misc</td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.openslr.org/104/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">150</td></tr><tr><td align="center">47</td><td align="center">nicolingua-west-african</td><td align="center">Multilingual</td><td align="center">Misc</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.13083.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.openslr.org/105/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">140 +</td></tr><tr><td align="center">48</td><td align="center">Samromur 21.05</td><td align="center">Samromur</td><td align="center">Misc</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/cadia-lvl/samromur-asr">[code]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.openslr.org/112/">[dataset]<i class="fas fa-external-link-alt"></i></a> <a class="link" target="_blank" rel="noopener" href="https://www.openslr.org/116/">[dataset]<i class="fas fa-external-link-alt"></i></a><a class="link" target="_blank" rel="noopener" href="https://www.openslr.org/117">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">145</td></tr><tr><td align="center">49</td><td align="center">Puebla-Nahuatl</td><td align="center">Puebla-Nahuatl</td><td align="center">Misc</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://aclanthology.org/2021.americasnlp-1.7.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.openslr.org/92/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">150 +</td></tr><tr><td align="center">50</td><td align="center">Golos</td><td align="center">Russian</td><td align="center">Misc</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.10161.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.openslr.org/114/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">1240</td></tr><tr><td align="center">51</td><td align="center">ParlaSpeech-HR</td><td align="center">Croatian</td><td align="center">Parliament</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://office.clarin.eu/v/CE-2021-1923-CLARIN2021_ConferenceProceedings.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.clarin.si/repository/xmlui/handle/11356/1494">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">1816</td></tr><tr><td align="center">52</td><td align="center">Lyon Corpus</td><td align="center">French</td><td align="center">Recording</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.mq.edu.au/__data/assets/pdf_file/0006/910077/2008Demuth-and-Tremblay.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://phonbank.talkbank.org/access/French/Lyon.html">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">185</td></tr><tr><td align="center">53</td><td align="center">Providence Corpus</td><td align="center">English</td><td align="center">Recording</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://d1wqtxts1xzle7.cloudfront.net/66484402/2006DemuthetalL_S-libre.pdf?1619046276=&response-content-disposition=inline;+filename=Word_minimality_Epenthesis_and_Coda_Lice.pdf&Expires=1652977685&Signature=d3EpWElGBNwVe6wvbA-Erk9bhbykEtwwSJN3JRcLRPU4dSB2iHz8FOjsYKf9YQVLQVHtNF-5L7EF325B7jWfaBXewazatJ9f-uC2qqQO~JPhD9GQgfTXims4pfu7cm1irdRT7fgYeqAbTT6xM9LMB0LdyMsevxB6tCJCX3IZwUdUaYsmNgm9iROxn7MZnr74gmQTekpRNK0AJFjpR261oYR5ORf8sgnpdVmjlbhlOTVraj12huOIvxEoIZ~QoFwA1mFSrLArBj83gdNVPvHpBFNoup4Dsejq1MbDOogFkoh~fW3C21xnjpM5PvUuq7SeT~gDgZQ~aZo14IS474pMtw__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://phonbank.talkbank.org/access/Eng-NA/Providence.html">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">364</td></tr><tr><td align="center">54</td><td align="center">CLARIN Spoken Corpora</td><td align="center">Czech</td><td align="center">Recording</td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.clarin.eu/resource-families/spoken-corpora">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">1120 +</td></tr><tr><td align="center">55</td><td align="center">Czech Parliament Plenary</td><td align="center">Czech</td><td align="center">Recording</td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-3126">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">444</td></tr><tr><td align="center">56</td><td align="center">(Youtube) Regional American Corpus</td><td align="center">English (Accented)</td><td align="center">Misc</td><td align="center"><a class="link" target="_blank" rel="noopener" href="http://cc.oulu.fi/~scoats/YouTube_Corpus_19a.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/stcoats/YouTube_Corpus">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">29k +</td></tr><tr><td align="center">57</td><td align="center">NISP Dataset</td><td align="center">Multilingual</td><td align="center">Recording</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2007.06021.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/iiscleap/NISP-Dataset?utm_source=catalyzex.com">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">56 +</td></tr><tr><td align="center">58</td><td align="center">Regional African American</td><td align="center">English (Accented)</td><td align="center">Recording</td><td align="center"><a class="link" target="_blank" rel="noopener" href="http://lingtools.uoregon.edu/coraal/userguide/CORAALUserGuide_current.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="http://lingtools.uoregon.edu/coraal/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">130 +</td></tr><tr><td align="center">59</td><td align="center">Indonesian Unsup</td><td align="center">Indonesian</td><td align="center">Misc</td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/Wikidepia/indonesian_datasets/tree/master/speech/unsupervised">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">unsup (3000+)</td></tr><tr><td align="center">60</td><td align="center">Librivox-Spanish</td><td align="center">Spanish</td><td align="center">Recording</td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/carlfm01/120h-spanish-speech">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">120</td></tr><tr><td align="center">61</td><td align="center">AVSpeech</td><td align="center">English</td><td align="center">Audio-Visual</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1804.03619.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://looking-to-listen.github.io/avspeech/download.html">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">unsup(4700)</td></tr><tr><td align="center">62</td><td align="center">CMLR</td><td align="center">Mandarin</td><td align="center">Audio-Visual</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1908.04917.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.vipazoo.cn/CMLR.html">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">100 +</td></tr><tr><td align="center">63</td><td align="center">Speech Accent Archive</td><td align="center">English</td><td align="center">Accented</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://brill.com/view/book/edcoll/9789401206884/B9789401206884-s014.xml">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="http://accent.gmu.edu/browse_language.php">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">TBC</td></tr><tr><td align="center">64</td><td align="center">BibleTTS</td><td align="center">Multilingual</td><td align="center">TTS</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.14456.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://masakhane-io.github.io/bibleTTS/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">86</td></tr><tr><td align="center">65</td><td align="center">NST-Norwegian</td><td align="center">Norwegian</td><td align="center">Recording</td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.nb.no/sprakbanken/en/resource-catalogue/oai-nb-no-sbr-54/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">540</td></tr><tr><td align="center">66</td><td align="center">NST-Danish</td><td align="center">Danish</td><td align="center">Recording</td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.nb.no/sprakbanken/en/resource-catalogue/oai-nb-no-sbr-55/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">500 +</td></tr><tr><td align="center">67</td><td align="center">NST-Swedish</td><td align="center">Swedish</td><td align="center">Recording</td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.nb.no/sprakbanken/en/resource-catalogue/oai-nb-no-sbr-56/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">300 +</td></tr><tr><td align="center">68</td><td align="center">NPSC</td><td align="center">Norwegian</td><td align="center">Parliament</td><td align="center"><a class="link" target="_blank" rel="noopener" href="http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.106.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.nb.no/sprakbanken/en/resource-catalogue/oai-nb-no-sbr-58/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">140</td></tr><tr><td align="center">69</td><td align="center">CI-AVSR</td><td align="center">Cantonese</td><td align="center">Audio-Visual</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2201.03804.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/HLTCHKUST/CI-AVSR">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">8 +</td></tr><tr><td align="center">70</td><td align="center">Aalto Finnish Parliament</td><td align="center">Finnish</td><td align="center">Parliament</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.14876.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.kielipankki.fi/corpora/fi-parliament-asr/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">3100 +</td></tr><tr><td align="center">71</td><td align="center">UserLibri</td><td align="center">English</td><td align="center">Reading</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.00706.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/google/userlibri">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">-</td></tr><tr><td align="center">72</td><td align="center">Ukrainian Speech</td><td align="center">Ukrainian</td><td align="center">Misc</td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/egorsmkv/speech-recognition-uk#-datasets">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">1300+</td></tr><tr><td align="center">73</td><td align="center">UCLA-ASR-corpus</td><td align="center">Multilingual</td><td align="center">Misc</td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/Open-Speech-EkStep/ULCA-asr-dataset-corpus">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">unsup(15k)<br>sup(9k)</td></tr><tr><td align="center">74</td><td align="center">ReazonSpeech</td><td align="center">Japanese</td><td align="center">Misc</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://research.reazon.jp/_static/reazonspeech_nlp2023.pdf">[paper]<i class="fas fa-external-link-alt"></i></a> <a class="link" target="_blank" rel="noopener" href="https://github.com/reazon-research/ReazonSpeech">[code]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://huggingface.co/datasets/reazon-research/reazonspeech">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">15k</td></tr><tr><td align="center">75</td><td align="center">Bundestag</td><td align="center">German</td><td align="center">Debate</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2302.06008v1.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://opendata.iisys.de/datasets.html#bundestag">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">sup(610)<br>unsup(1038)</td></tr></tbody></table><h3 id="need-application"><a href="#need-application" class="headerlink" title="need application"></a>need application</h3><table><thead><tr><th align="center"><strong>id</strong></th><th align="center"><strong>Name</strong></th><th align="center"><strong>Language</strong></th><th align="center"><strong>Type/Domain</strong></th><th align="center"><strong>Paper Link</strong></th><th align="center"><strong>Data Link</strong></th><th align="center"><strong>Size (Hours)</strong></th></tr></thead><tbody><tr><td align="center">1</td><td align="center">Fisher</td><td align="center">English</td><td align="center">Conversational</td><td align="center"><a class="link" target="_blank" rel="noopener" href="http://www.lrec-conf.org/proceedings/lrec2004/pdf/767.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://catalog.ldc.upenn.edu/search">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">2000</td></tr><tr><td align="center">2</td><td align="center">WenetSpeech</td><td align="center">Mandarin</td><td align="center">Misc</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2110.03370.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.openslr.org/121">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">sup(10k)<br>weaksup(2.4k)<br>unsup(10k)</td></tr><tr><td align="center">3</td><td align="center">aishell-2</td><td align="center">Mandarin</td><td align="center">Recording</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1808.10583.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.aishelltech.com/aishell_2">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">1000</td></tr><tr><td align="center">4</td><td align="center">aidatatang_1505zh</td><td align="center">Mandarin</td><td align="center">Recording</td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.datatang.com/opensource">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">1505</td></tr><tr><td align="center">5</td><td align="center">SLT 2021 CSRC</td><td align="center">Mandarin</td><td align="center">Misc</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2011.06724.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.data-baker.com/csrc_challenge.html">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">400</td></tr><tr><td align="center">6</td><td align="center">GigaSpeech</td><td align="center">English</td><td align="center">Misc</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.06909.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/SpeechColab/GigaSpeech">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">sup(10k)<br>unsup(23k)</td></tr><tr><td align="center">7</td><td align="center">SPGISpeech</td><td align="center">English</td><td align="center">Misc</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.02014.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://datasets.kensho.com/datasets/spgispeech">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">5000</td></tr><tr><td align="center">8</td><td align="center">AESRC 2020</td><td align="center">English (accented)</td><td align="center">Misc</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2102.10233.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.datatang.com/INTERSPEECH2020">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">160</td></tr><tr><td align="center">9</td><td align="center">LaboroTVSpeech</td><td align="center">Japanese</td><td align="center">Misc</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.14736.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://laboro.ai/activity/column/engineer/eg-laboro-tv-corpus-jp/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">2000 +</td></tr><tr><td align="center">10</td><td align="center">TAL_CSASR</td><td align="center">Mandarin-English CS</td><td align="center">Lectures</td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://ai.100tal.com/dataset">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">587</td></tr><tr><td align="center">11</td><td align="center">ASRU 2019 ASR</td><td align="center">Mandarin-English CS</td><td align="center">Reading</td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.datatang.com/competition">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">700 +</td></tr><tr><td align="center">12</td><td align="center">SEAME</td><td align="center">Mandarin-English CS</td><td align="center">Recording</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.isca-speech.org/archive/pdfs/interspeech_2010/lyu10_interspeech.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://catalog.ldc.upenn.edu/LDC2015S04">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">196</td></tr><tr><td align="center">13</td><td align="center">Fearless Steps</td><td align="center">English</td><td align="center">Misc</td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://fearless-steps.github.io/ChallengePhase3/#19k_Corpus_Access">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">unsup(19k)</td></tr><tr><td align="center">14</td><td align="center">FTSpeech</td><td align="center">Danish</td><td align="center">Meeting</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2005.12368.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://ftspeech.github.io/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">1800 +</td></tr><tr><td align="center">15</td><td align="center">KeSpeech</td><td align="center">Mandarin</td><td align="center">Recording</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://openreview.net/pdf?id=b3Zoeq2sCLq">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/KeSpeech/KeSpeech">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">1542</td></tr><tr><td align="center">16</td><td align="center">KsponSpeech</td><td align="center">Korean</td><td align="center">Conversational</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.mdpi.com/2076-3417/10/19/6936">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://huggingface.co/datasets/cheulyop/ksponspeech">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">969</td></tr><tr><td align="center">17</td><td align="center">RVTE database</td><td align="center">Spanish</td><td align="center">TV</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://catedrartve.unizar.es/reto2022/RTVE2022DB.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://catedrartve.unizar.es/rtvedatabase.html">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">800 +</td></tr><tr><td align="center">18</td><td align="center">DiDiSpeech</td><td align="center">Mandarin</td><td align="center">Recording</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2010.09275.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/athena-team/DiDiSpeech">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">800</td></tr><tr><td align="center">19</td><td align="center">Babel</td><td align="center">Multilingual</td><td align="center">Telephone</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://eprints.whiterose.ac.uk/152840/8/Gales%20et%20al%202014.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.nist.gov/itl/iad/mig/openkws16-evaluation">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">1000 +</td></tr><tr><td align="center">20</td><td align="center">National Speech Corpus</td><td align="center">English (Singapore)</td><td align="center">Misc</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.isca-speech.org/archive_v0/Interspeech_2019/pdfs/1525.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.imda.gov.sg/programme-listing/digital-services-lab/national-speech-corpus">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">3000 +</td></tr><tr><td align="center">21</td><td align="center">MyST Children’s Speech</td><td align="center">English</td><td align="center">Recording</td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="http://boulderlearning.com/request-the-myst-corpus/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">393</td></tr><tr><td align="center">22</td><td align="center">L2-ARCTIC</td><td align="center">L2 English</td><td align="center">Recording</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.isca-speech.org/archive_v0/Interspeech_2018/pdfs/1110.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://psi.engr.tamu.edu/l2-arctic-corpus/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">20 +</td></tr><tr><td align="center">23</td><td align="center">JSpeech</td><td align="center">Multilingual</td><td align="center">Recording</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8639658">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/miras-tech/jspeech">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">1332 +</td></tr><tr><td align="center">24</td><td align="center">LRS2-BBC</td><td align="center">English</td><td align="center">Audio-Visual</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1809.02108.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs2.html">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">220 +</td></tr><tr><td align="center">25</td><td align="center">LRS3-TED</td><td align="center">English</td><td align="center">Audio-Visual</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1809.00496.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs3.html">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">470 +</td></tr><tr><td align="center">26</td><td align="center">LRS3-Lang</td><td align="center">Multilingual</td><td align="center">Audio-Visual</td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs3-lang.html">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">1300 +</td></tr><tr><td align="center">27</td><td align="center">QASR</td><td align="center">Arabic</td><td align="center">Dialects</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.13000.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arabicspeech.org/qasr/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">2000 +</td></tr><tr><td align="center">28</td><td align="center">ADI (MGB-5)</td><td align="center">Arabic</td><td align="center">Dialects</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://swshon.github.io/pdf/ali_asru2019_mgb5.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arabicspeech.org/mgb5/#adi17">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">unsup (3000 +)</td></tr><tr><td align="center">29</td><td align="center">MGB-2</td><td align="center">Arabic</td><td align="center">TV</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/1609.05625.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="http://www.mgb-challenge.org/MGB-2.html">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">1200 +</td></tr><tr><td align="center">30</td><td align="center">3MASSIV</td><td align="center">Multilingual</td><td align="center">Audio-Visual</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Gupta_3MASSIV_Multilingual_Multimodal_and_Multi-Aspect_Dataset_of_Social_Media_Short_CVPR_2022_paper.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/ShareChatAI/3MASSIV">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">sup(310)<br>unsup(600)</td></tr><tr><td align="center">31</td><td align="center">MDCC</td><td align="center">Cantonese</td><td align="center">Misc</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2201.02419.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/HLTCHKUST/cantonese-asr">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">73 +</td></tr><tr><td align="center">32</td><td align="center">Lahjoita Puhetta</td><td align="center">Finnish</td><td align="center">Misc</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.12906.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/aalto-speech/lahjoita-puhetta-resources">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">sup(1600)<br>unsup(2000)</td></tr><tr><td align="center">33</td><td align="center">SDS-200</td><td align="center">Swiss German</td><td align="center">Dialects</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.09501.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://swissnlp.org/datasets/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">200</td></tr><tr><td align="center">34</td><td align="center">Modality Corpus</td><td align="center">Multilingual</td><td align="center">Audio-Visual</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://link.springer.com/content/pdf/10.1007/s10844-016-0438-z.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="http://www.modality-corpus.org/">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">30 +</td></tr><tr><td align="center">35</td><td align="center">Hindi-Tamil-English</td><td align="center">Multilingual</td><td align="center">Misc</td><td align="center">-</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://sites.google.com/view/indian-language-asrchallenge/home">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">690</td></tr><tr><td align="center">36</td><td align="center">English-Vietnamese Corpus</td><td align="center">English, Vietnamese</td><td align="center">Misc</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2208.04243.pdf">[paper]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://github.com/VinAIResearch/PhoST">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">500+</td></tr><tr><td align="center">37</td><td align="center">OLKAVS</td><td align="center">Korean</td><td align="center">Audio-Visual</td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://arxiv.org/pdf/2301.06375.pdf">[paper]<i class="fas fa-external-link-alt"></i></a> <a class="link" target="_blank" rel="noopener" href="https://github.com/IIP-Sogang/olkavs-avspeech">[code]<i class="fas fa-external-link-alt"></i></a></td><td align="center"><a class="link" target="_blank" rel="noopener" href="https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=538">[dataset]<i class="fas fa-external-link-alt"></i></a></td><td align="center">1150</td></tr></tbody></table><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a class="link" target="_blank" rel="noopener" href="https://github.com/coqui-ai/open-speech-corpora">https://github.com/coqui-ai/open-speech-corpora<i class="fas fa-external-link-alt"></i></a></li><li><a class="link" target="_blank" rel="noopener" href="https://openslr.org/resources.php">https://openslr.org/resources.php<i class="fas fa-external-link-alt"></i></a></li></ul></div><div class="post-copyright-info"><div class="article-copyright-info-container"><ul class="copyright-info-content"><li class="post-title"><span class="type">本文标题</span>：<span class="content">数据集 | 开源语音数据库汇总</span></li><li class="post-author"><span class="type">本文作者</span>：<span class="content">RevoSpeech</span></li><li class="post-time"><span class="type">创建时间</span>：<span class="content">2023-01-07</span></li><li class="post-link"><span class="type">本文链接</span>：<span class="content">2023/01/07/speech-datasets-collection/</span></li><li class="post-license"><span class="type">版权声明</span>：<span class="content">本博客所有文章除特别声明外，均采用 <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">BY-NC-SA</a> 许可协议。转载请注明出处！</span></li></ul><div class="copy-copyright-info flex-center tooltip" data-content="复制版权信息" data-offset-y="-2px"><i class="fa-solid fa-copy"></i></div></div></div><ul class="post-tags-box"><li class="tag-item"><a href="/tags/%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90/">#语音合成</a>&nbsp;</li><li class="tag-item"><a href="/tags/%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB/">#语音识别</a>&nbsp;</li></ul><div class="article-nav"><div class="article-prev"><a class="prev" rel="prev" href="/2023/01/14/lyra_v2_soundstream/"><span class="left arrow-icon flex-center"><i class="fas fa-chevron-left"></i> </span><span class="title flex-center"><span class="post-nav-title-item">音频编解码 | SoundStream</span> <span class="post-nav-item">上一篇</span></span></a></div><div class="article-next"><a class="next" rel="next" href="/2023/01/01/paper_list/"><span class="title flex-center"><span class="post-nav-title-item">快速索引 | 论文笔记清单</span> <span class="post-nav-item">下一篇</span> </span><span class="right arrow-icon flex-center"><i class="fas fa-chevron-right"></i></span></a></div></div></div><div class="toc-content-container"><div class="post-toc-wrap"><div class="post-toc"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Data-Overview"><span class="nav-number">1.</span> <span class="nav-text">Data Overview</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#List-of-ASR-corpora"><span class="nav-number">2.</span> <span class="nav-text">List of ASR corpora</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#directly-downloadable"><span class="nav-number">2.1.</span> <span class="nav-text">directly downloadable</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#need-application"><span class="nav-number">2.2.</span> <span class="nav-text">need application</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-number">3.</span> <span class="nav-text">References</span></a></li></ol></div></div></div></div></div></div></div><div class="page-main-content-bottom"><footer class="footer"><div class="info-container"><div class="copyright-info info-item">&copy; <span>2023</span> - 2023 &nbsp;<i class="fas fa-heart icon-animate"></i> &nbsp;<a href="/">RevoSpeech</a></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="website-count info-item">总字数&nbsp;25.4k 总访问量&nbsp;<span id="busuanzi_value_site_pv"></span></div><div class="theme-info info-item">由 <a target="_blank" href="https://hexo.io">Hexo</a> 驱动&nbsp;|&nbsp;主题&nbsp;<a class="theme-version" target="_blank" href="https://github.com/XPoet/hexo-theme-keep">Keep v3.6.1</a></div></div></footer></div></div><div class="post-tools"><div class="post-tools-container"><ul class="tools-list"><li class="tools-item flex-center toggle-show-toc"><i class="fas fa-list"></i></li></ul></div></div><div class="right-bottom-side-tools"><div class="side-tools-container"><ul class="side-tools-list"><li class="tools-item tool-font-adjust-plus flex-center"><i class="fas fa-search-plus"></i></li><li class="tools-item tool-font-adjust-minus flex-center"><i class="fas fa-search-minus"></i></li><li class="tools-item tool-dark-light-toggle flex-center"><i class="fas fa-moon"></i></li><li class="tools-item rss flex-center"><a class="flex-center" href="/atom.xml" target="_blank"><i class="fas fa-rss"></i></a></li><li class="tools-item tool-scroll-to-bottom flex-center"><i class="fas fa-arrow-down"></i></li></ul><ul class="exposed-tools-list"><li class="tools-item tool-toggle-show flex-center"><i class="fas fa-cog fa-spin"></i></li><li class="tools-item tool-scroll-to-top flex-center"><i class="arrow-up fas fa-arrow-up"></i> <span class="percent"></span></li></ul></div></div><div class="zoom-in-image-mask"><img class="zoom-in-image"></div><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-input-field-pre"><i class="fas fa-keyboard"></i></span><div class="search-input-container"><input autocomplete="off" autocorrect="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="close-popup-btn"><i class="fas fa-times"></i></span></div><div id="search-result"><div id="no-result"><i class="fas fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></main><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/utils.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/main.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/header-shrink.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/back2top.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/dark-light-toggle.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/local-search.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/code-block.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/lazyload.js"></script><div class="post-scripts pjax"><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/post-helper.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/libs/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/toc.js"></script></div><script src="//cdn.jsdelivr.net/npm/hexo-theme-keep@3.6.1/source/js/libs/pjax.min.js"></script><script>window.addEventListener("DOMContentLoaded",()=>{window.pjax=new Pjax({selectors:["head title",".page-container",".pjax"],history:!0,debug:!1,cacheBust:!1,timeout:0,analytics:!1,currentUrlFullReload:!1,scrollRestoration:!1}),document.addEventListener("pjax:send",()=>{KEEP.utils.pjaxProgressBarStart()}),document.addEventListener("pjax:complete",()=>{KEEP.utils.pjaxProgressBarEnd(),window.pjax.executeScripts(document.querySelectorAll("script[data-pjax], .pjax script")),KEEP.refresh()})})</script></body></html>